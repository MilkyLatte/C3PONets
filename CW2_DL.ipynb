{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CM50265 Machine Learning 2\n",
    "\n",
    "## Coursework 2: Deep learning\n",
    "### Youssef Alami Mejjati, Jordan Taylor, Jake Deane and Mohammad Golbabaee\n",
    "\n",
    "\n",
    "This coursework is worth 75 points from the overall mark of 100 for this unit. Marks are given beside each task. The report will be your main method of assessment. __Students should form groups of 3 individuals and submit one report per group.__ For this coursework students can choose their partners until __20th February__.  Each group will receive a total mark for this course work and individuals in that group will share this mark (with weights) according to their contributions.  \n",
    "\n",
    "- The __submission deadline__ for your report is\n",
    "__17th May 2020, 12:00: online Moodle submission of your final report__\n",
    "\n",
    "The main part of your report should not exceed __3000 word limit__. After the main part please attach a Table of individuals’ contributions and appendices including ONLY the codes (these are excluded from the word limit). The report should be submitted in PDF format. __Table of contributions__ should include both students’ names/university IDs, the list of contributions of each student, and finally the contribution percentage for each student. This percentage should be agreed between both individuals and it will be used to weigh their marks. We highly encourage individuals to evenly share the workload. Further, you should __include all codes__ relevant to your implementations as an appendix or appendices with a clear referencing to the main body of your report. Codes must be well commented (indicating major steps).\n",
    "\n",
    "First and foremost your report should demonstrate and evaluate your results. It must include figures and screenshots with appropriate resolutions. Evaluation can be qualitative (how it looks) and, wherever possible, quantitative (tested numerically). Second you should provide evidence that you understood the mathematics behind the assignment in each task/question. You should concisely explain your approach to solve each task/question, and the choices you make (e.g. hyper-parameters etc) for each part.\n",
    "\n",
    "Usual university rules apply, please check your MSc program handbook, particularly regarding plagiarism and delayed deliveries. \n",
    "\n",
    "__Note:__ All tasks should be implemented in TensorFlow. Guidelines below (tasks 3-5, 7) should work for a TensorFlow version 1. If you use TF v2 please note that some syntaxes have changed during the recent update, but you can still use version compatiblity to resolve the issue. The list includes but might not be limited to:\n",
    "\n",
    "tf.layers.conv2d -> tf.nn.conv2d\n",
    "\n",
    "tf.layers.dense -> tf.nn.dense\n",
    "\n",
    "tf.layers.flatten -> tf.compat.v1.layers.flatten\n",
    "\n",
    "tf.variable_scope -> tf.compat.v1.variable_scope\n",
    "\n",
    "tf.get_collection -> tf.compat.v1.get_collection\n",
    "\n",
    "similarly for tf.compat.v1.InteractiveSession, tf.compat.v1.train.AdamOptimizer, tf.compat.v1.placeholder, tf.compat.v1.global_variables_initializer, tf.compat.v1.local_variables_initializer, tf.compat.v1.train.Saver, tf.compat.v1.summary.FileWriter, tf.compat.v1.trainable_variables\n",
    "\n",
    "See TF web documentation in case you encounter version incompatibility. \n",
    "\n",
    "Guides on tasks 6 and 8 should be already compatible with TF2. \n",
    "\n",
    "\n",
    "## The coursework \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.disable_eager_execution()\n",
    "except:\n",
    "    import tensorflow as tf\n",
    "    \n",
    "# import tensorflow as tf # This may laod Tensorflow 2.0.0 if that is the verison of the package on your distribution\n",
    "#use import tensorflow.compat.v1 as tf for tensorflow 1\n",
    "# tf.disable_v2_behaviour() # Only needed if using tensorflow 1 with tensorflow 2 being the package avlaiable  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import Image\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "n_train = x_train.shape[0]\n",
    "n_test = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the data to be in $[-1,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 127.5 - 1, x_test / 127.5 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of each data points is $28 \\times 28$. While this format will be useful when using CNNs, we will vectorize the datapoints for visualization and preliminary questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_features = np.prod(x_train.shape[1:])\n",
    "x_train.resize((n_train, nb_features))\n",
    "x_test.resize((n_test, nb_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data visualisation (5 points)\n",
    "Project the training data points in a 2D space using PCA. Use the obtained 2D embedding and plot the training data-points with different markers or colors for each class (you are allowed to use PCA from scikit learn). \n",
    "- Why PCA is a good option to visualize data? \n",
    "- Add this plot in your report and discuss your observations. \n",
    "- Which classes can be linearly separated?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Perceptrons: (10 points)\n",
    "Single Layer Perceptron is one of the most basic binary classifiers one can use. In this part of the CW you should implement an iterative algorithm for training the Single Layer Perceptron.\n",
    "\n",
    "As we are dealing with a binary classification problem, we will pick data points corresponding to classes 0 and 1 (handwritten digits). In addition we choose our binary labels to be -1 and 1, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = (y_train == 0) + (y_train == 1)\n",
    "binary_x_train = x_train[cond,:]\n",
    "binary_y_train = y_train[cond]*1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_y_train[binary_y_train == 0] = -1\n",
    "binary_y_train[binary_y_train == 1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.1\n",
    "Complete the function 'predict' below.\n",
    "#### inputs:\n",
    "\n",
    "+ $x\\in\\mathbb{R^{n*m}}$, with $n$ being the number of datapoints and $m$ being the feature dimensionality. \n",
    "+ $w \\in\\mathbb{R^m}$ is the parameter vector we wish to learn. \n",
    "+ $b \\in\\mathbb{R}$ is the corresponding bias.\n",
    "\n",
    "#### outputs: \n",
    "+ 'prediction'$\\in\\mathbb{R^n}$, a vector containing prediction values associated with $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    ######### Complete the function- x point ######### \n",
    "    linear_comb = np.dot(x, w) + b\n",
    "    if linear_comb > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.2\n",
    "+ Use the funtion 'predict' above to implement the Single Layer Perceptron algorithm by completing the function 'optimize' defined below.\n",
    "    #### inputs:\n",
    "\n",
    "    + $x\\in\\mathbb{R^{n*m}}$, with $n$ being the number of datapoints and $m$ being the feature dimensionality. \n",
    "    + $w \\in\\mathbb{R^m}$ is the initial parameter vector.\n",
    "    + $b \\in\\mathbb{R}$ is the initial bias value.\n",
    "    + $y\\in\\mathbb{R^n}$ is the training labels associated with x.\n",
    "    #### outputs:\n",
    "    + $w$ is the optimized parameter vector.\n",
    "    + $b$ the corresponding learned bias.\n",
    "    + $\\text{error}$ is the classification error obtained.  \n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "Use the learned parameters $w$, $b$ (obtained via function 'optimize') and the function 'predict' to return the classification accuracy on the test set using x_train and y_train as training data. \n",
    "\n",
    "- Demonstrate that your algorithm converges to a good local minima. Plot the training error curve vs. number of iterations. \n",
    "- Show what feature $w$ has learned and discuss why? (demonstrate $w$ as an image with the same size as inputs).\n",
    "\n",
    "- Repeat this training/testing procedure to classify different pairs. Report the accuracies of 5 pairs in a Table and dicuss why some are easier to classify than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_many(x, w, b):\n",
    "    pred = np.empty(len(x))\n",
    "    for i in range(len(x)):\n",
    "        pred[i] = predict(x[i], w, b)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(pred, y):\n",
    "    loss = 0\n",
    "    for pred_i, y_i in zip(pred, y):\n",
    "        if pred_i != y_i:\n",
    "            loss += 1\n",
    "    return loss/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(x, y, lr=0.003):\n",
    "    iter = 0\n",
    "    error=np.inf\n",
    "    n, m = x.shape\n",
    "    w = np.random.rand(m) # Initialize the w vector\n",
    "    b = np.random.rand()  # Initialize the b vector\n",
    "    hist = []\n",
    "    while (iter <= 1000) & (error > 1e-3):\n",
    "        idx = np.random.randint(n) # Get random index to update\n",
    "        prediction = predict(x[idx], w, b) # Predict one to update the bias\n",
    "        if prediction == y[idx]:\n",
    "            continue\n",
    "        err = y[idx] - prediction\n",
    "        w = w + lr * y[idx] * x[idx] # Update weights\n",
    "        b += lr * err # Update bias\n",
    "        if iter % 1 == 0: # Change when the loss is recorded and calculated\n",
    "            pred = predict_many(x, w, b)\n",
    "            error = loss(pred, y) # Update error\n",
    "            hist.append(error)\n",
    "        iter += 1\n",
    "    return w, b, error, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_w, the_b, training_error, hist = optimize(binary_x_train, binary_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc825884550>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((the_w + the_b).reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc82592f9b0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADN1JREFUeJzt3X+s3XV9x/Hn23pbtLiF4qwN1OEIaBjJirtDnQx1iEHCLPyxSs1MtxCrmWxjcckI+0P+cFmjE0fioimjUjZFFwHhD5xiM0cMjHFhHVC6yY8VaVMoBDbBhXKh7/1xv5AL3PM9l/P79v18JDfnnO/7+z3fd77pq9/vOZ9zzicyE0n1vG7cDUgaD8MvFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqmo149yZ8tjRR7BylHuUirlWX7Oc3kwFrNuX+GPiLOAy4FlwN9l5pa29Y9gJe+OM/rZpaQWt+eORa/b82V/RCwD/hb4CHASsDEiTur1+SSNVj+v+U8FHsjMhzLzOeBbwPrBtCVp2PoJ/zHAI/Me722WvUxEbI6ImYiYmeVgH7uTNEhDf7c/M7dm5nRmTk+xYti7k7RI/YR/H7B23uNjm2WSloB+wn8HcEJEvD0ilgPnAzcOpi1Jw9bzUF9mPh8RFwLfZ26ob1tm7hpYZ5KGqq9x/sy8CbhpQL1IGiE/3isVZfilogy/VJThl4oy/FJRhl8qyvBLRRl+qSjDLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8VZfilogy/VJThl4oy/FJRfc3SGxF7gKeBF4DnM3N6EE1JAA9+8b2t9d0f/0prfSqWdayd/oebW7d9w3f/rbV+OOgr/I0PZuYTA3geSSPkZb9UVL/hT+AHEXFnRLRfR0maKP1e9p+Wmfsi4i3AzRHxn5l5y/wVmv8UNgMcwRv73J2kQenrzJ+Z+5rbA8D1wKkLrLM1M6czc3qKFf3sTtIA9Rz+iFgZEW968T7wYeDeQTUmabj6uexfDVwfES8+zzcz858G0pWkoes5/Jn5EPBrA+xFxTz6p7/ZWv/Rx77QWp/N5b3vPHvf9HDhUJ9UlOGXijL8UlGGXyrK8EtFGX6pqEF8q0/qyTNrD7XWV72uj6E8deWZXyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeKcpxfQ/XM7767Y+3a8y7vsnW0Vr/2P+9srf9wQ+dfkl/58K7Wbds/gXB48MwvFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0U5zq++PHvOqyZpepnP/dW2jrUTp9rH8bvZfsVZrfW33ndrX89/uPPMLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFdR3nj4htwDnAgcw8uVm2Cvg2cBywB9iQmU8Nr01Nqv2/92xr/YNvaKsva912054Ptdbfernj+P1YzJn/KuCVn6a4GNiRmScAO5rHkpaQruHPzFuAJ1+xeD2wvbm/HTh3wH1JGrJeX/Ovzsz9zf1HgdUD6kfSiPT9hl9mJpCd6hGxOSJmImJmloP97k7SgPQa/sciYg1Ac3ug04qZuTUzpzNzeooVPe5O0qD1Gv4bgU3N/U3ADYNpR9KodA1/RFwD3Aa8IyL2RsQFwBbgzIi4H/hQ81jSEtJ1nD8zN3YonTHgXjSBXn/sMa31Xb/19db6bL7QsbZ7tn3fP73sxNb6Sm5vfwK18hN+UlGGXyrK8EtFGX6pKMMvFWX4paL86e7ilv3qO1rr09+8d2j7/th1f9xaP/7afx3avuWZXyrL8EtFGX6pKMMvFWX4paIMv1SU4ZeKcpy/uIc/enRr/TtH/3uXZ2j/+e2PP/g7HWsnbnmwddvOXwbWIHjml4oy/FJRhl8qyvBLRRl+qSjDLxVl+KWiHOc/zD35B+9trV//6S92eYap1uqnH3l/a312U+dZml54/Kdd9q1h8swvFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0V1HeePiG3AOcCBzDy5WXYp8Eng8Wa1SzLzpmE1qXZtv71/6+e/0mXrI/ra9217j2utr90zvN/9V38Wc+a/CjhrgeVfzsx1zZ/Bl5aYruHPzFuAJ0fQi6QR6uc1/4URcXdEbIuIowbWkaSR6DX8XwWOB9YB+4EvdVoxIjZHxExEzMxysMfdSRq0nsKfmY9l5guZeQi4Aji1Zd2tmTmdmdNTdP6Sh6TR6in8EbFm3sPzAN/SlZaYxQz1XQN8AHhzROwFPgd8ICLWAQnsAT41xB4lDUHX8GfmxgUWXzmEXtSjn1zyxo612Rzur9+/bUt7PYe6d/XDT/hJRRl+qSjDLxVl+KWiDL9UlOGXivKnu5eAQ+8/pbX++envDm3fZ957fmv9yBk/37VUeeaXijL8UlGGXyrK8EtFGX6pKMMvFWX4paIc518C/vKqra31k6d6/+Lsn+0/vbX+ixufaq0P9wvDGibP/FJRhl8qyvBLRRl+qSjDLxVl+KWiDL9UlOP8S8Apy9v/j+7n57lv+/q7WutveerWnp9bk80zv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8V1XWcPyLWAlcDq5mbcXlrZl4eEauAbwPHAXuADZnZ/uVvLeiR75zcWp+KnUPb95ofPdFa9/v6h6/FnPmfBz6bmScB7wE+ExEnARcDOzLzBGBH81jSEtE1/Jm5PzPvau4/DewGjgHWA9ub1bYD5w6rSUmD95pe80fEccApwO3A6szc35QeZe5lgaQlYtHhj4gjgWuBizLzZ/NrmZnMvR+w0HabI2ImImZmOdhXs5IGZ1Hhj4gp5oL/jcy8rln8WESsaeprgAMLbZuZWzNzOjOnp1gxiJ4lDUDX8EdEAFcCuzPzsnmlG4FNzf1NwA2Db0/SsCzmK73vAz4B3BPx0pjTJcAW4B8j4gLgYWDDcFpc+rpNsf036/6htd7tK7v/e+jZjrXf+N5Frdu+8+H7Wus6fHUNf2b+GIgO5TMG246kUfETflJRhl8qyvBLRRl+qSjDLxVl+KWi/OnuEXh21fLW+mlH/LzLMyxrrX7//97WsXbi5jtatz3UZc86fHnml4oy/FJRhl8qyvBLRRl+qSjDLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4paL8Pv8I/MLOR1vrf7T3t1vrX1v7L4NsRwI880tlGX6pKMMvFWX4paIMv1SU4ZeKMvxSUV3H+SNiLXA1sBpIYGtmXh4RlwKfBB5vVr0kM28aVqNL2fP//XBrfe972rc/h18fYDfSnMV8yOd54LOZeVdEvAm4MyJubmpfzsy/Hl57koala/gzcz+wv7n/dETsBo4ZdmOShus1veaPiOOAU4Dbm0UXRsTdEbEtIo7qsM3miJiJiJlZDvbVrKTBWXT4I+JI4Frgosz8GfBV4HhgHXNXBl9aaLvM3JqZ05k5PcWKAbQsaRAWFf6ImGIu+N/IzOsAMvOxzHwhMw8BVwCnDq9NSYPWNfwREcCVwO7MvGze8jXzVjsPuHfw7UkalsW82/8+4BPAPRGxs1l2CbAxItYxN/y3B/jUUDqUNBSLebf/x0AsUHJMX1rC/ISfVJThl4oy/FJRhl8qyvBLRRl+qSjDLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFGX6pqMjM0e0s4nFg/u9Yvxl4YmQNvDaT2tuk9gX21qtB9vbLmflLi1lxpOF/1c4jZjJzemwNtJjU3ia1L7C3Xo2rNy/7paIMv1TUuMO/dcz7bzOpvU1qX2BvvRpLb2N9zS9pfMZ95pc0JmMJf0ScFRH/FREPRMTF4+ihk4jYExH3RMTOiJgZcy/bIuJARNw7b9mqiLg5Iu5vbhecJm1MvV0aEfuaY7czIs4eU29rI+KfI+K+iNgVEX/SLB/rsWvpayzHbeSX/RGxDPgJcCawF7gD2JiZ9420kQ4iYg8wnZljHxOOiNOBZ4CrM/PkZtkXgCczc0vzH+dRmfnnE9LbpcAz4565uZlQZs38maWBc4HfZ4zHrqWvDYzhuI3jzH8q8EBmPpSZzwHfAtaPoY+Jl5m3AE++YvF6YHtzfztz/3hGrkNvEyEz92fmXc39p4EXZ5Ye67Fr6WssxhH+Y4BH5j3ey2RN+Z3ADyLizojYPO5mFrC6mTYd4FFg9TibWUDXmZtH6RUzS0/MsetlxutB8w2/VzstM98FfAT4THN5O5Fy7jXbJA3XLGrm5lFZYGbpl4zz2PU64/WgjSP8+4C18x4f2yybCJm5r7k9AFzP5M0+/NiLk6Q2twfG3M9LJmnm5oVmlmYCjt0kzXg9jvDfAZwQEW+PiOXA+cCNY+jjVSJiZfNGDBGxEvgwkzf78I3Apub+JuCGMfbyMpMyc3OnmaUZ87GbuBmvM3Pkf8DZzL3j/yDwF+PooUNfvwL8R/O3a9y9Adcwdxk4y9x7IxcARwM7gPuBHwKrJqi3vwfuAe5mLmhrxtTbacxd0t8N7Gz+zh73sWvpayzHzU/4SUX5hp9UlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4paL+H5OL6YVERhITAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(binary_x_train[1].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Training error')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XmcXFWZ//HP09X7lp0OSQcS1hC2QAIICHYUIaiACwqoCApGVMYNYQAHVMYZUOfHjAg6RAdcMSCoBIygQBoEIWQxLNkXsu97eu/qen5/3NtNpemlutPVtfT3/XrVq+veOvfe51TdrqfOucsxd0dERAQgJ9UBiIhI+lBSEBGRNkoKIiLSRklBRETaKCmIiEgbJQUREWmjpJBFzCxiZjVmdlhflpXUMbPfmNl3UrRtM7NfmdkeM/tHB69fZWZ/SUVscTH83MxuTWUM2UZJIYXCL+XWR8zM6uOmP9XT9bl7i7uXuvu6viwrbzOzDWa22cyK4+ZdZ2bPpDKuJKkC3gOMcvez2r/o7r909wsBzCzXzNzMxiYrGDO71syq28Vwrbv/Z7K2ORApKaRQ+KVc6u6lwDrgorh5v21f3sxy+z/K5OqoTj2tZwrel3zg+n7e5kEzs0gPFzkceMvd65IRT7xs3LczlZJCGjOz75nZw2b2OzPbD3zazM40s1fCJv1mM7vHzPLC8gf8Wgu7Hu4xs7+Y2X4ze9nMxvW0bPj6hWa23Mz2mtmPzewlM7u6k7hzzOxWM1tlZjvMbIaZDQlfOyrc7mfNbB3w147mhWU/YmaLwro+Z2bHxm1jg5ndaGZvALUdxPAzM7ur3bw/m9lXwue3mtkmM9tnZkvNrKoHH80PgJvMrLyD7R5lZt5u3out71X4a/f58L3eY2YrzewMM7vGzNab2VYz+3S71Y4ws2fDz2W2mY2JW/cEM3vGzHaF9fhY3Gu/MbP7zOwpM6sFzukg3kozezJcfoWZfS6cPw34X+CcsOV6WwfLxv9yfyH8uygs/7GwzMVm9lpY1xfN7IS45d/xGZrZv5nZ6rCui8zs4nD+icC9cfHsiKvjd+LWeV34nu40sz+Z2aHh/Nb9/Qvh67vN7J645Y4xsxfC/XuHmT3Uvr4DhrvrkQYPYA1wXrt53wOagIsIEngRcBpwBpALHAEsB64Py+cCDowNp38D7AAmA3nAw8BvelH2EGA/cEn42jeAZuDqTupyA/ASMBooBH4O/Dp87ahwuw8CxWGdOpp3HFADvDfc5q3AMiAvXM8GYD5QCRR1EMN7w/fUwulhQD1QARwPrAVGhq+NA45I8HPaQNCtMhP4TjjvOuCZ+Pq1W+bF1vcKuDZ8764EIsBdYSz3AAXAB4C9QHHc57IXODt8/T6gOnytFNgIfCb8PCcBO4Fj45bdDZxJsP8UdFCfl4Afh5/TqeE+8J64WKu7eC+ujYvlgP0pnHcasDX8GwE+B6wC8jv7DIFPAIeG8X4y3AcqOosnrGPr53A+sA2YGNbnJ8Bz7eJ7HBgEjAV2Ef7PAb8H/jXcbiFwdqq/E1L1UEsh/b3o7k+4e8zd6919rrvPcfeou68GphP0+3bmUXef5+7NwG8J/mF6WvZDwEJ3fzx87b8Jvjw6cx1wq7tvdPcG4LvAx80sfn/7trvXuXt9J/MuB2a6+3PhNu8i+Gc+I678j9x9Q7t1tKomSCZnhtOfAP7u7luBKME//vFmluvub4XvZU/cBnzNzIb2cDmAle7+a3dvIUi+hwHfdfdGd58VljkirvwT7v6SuzcSJMdzw1/AlwDL3f1X4f4wH/gTcGncsn9095fD/acxPoiwJXg6cLO7N7j7AoLEfGUv6tSRacBPwn22xd0fCOefFlfmgM/Q3R9x981hvA8RJPbJCW7vU8DP3X1huN/dDLzHzCrjytzp7nvdfQ3BPtK6jzcTJIpDw/fipR7XNksoKaS/9fETZjY+7AbZYmb7gDuA4V0svyXueR3Br8uelh0VH4cHP602dLGew4Anwi6DPcAb4fxD4sqsf+diB8wbRfALunWbsXCbo7tZR3z5h4ErwlmfJEh0uPsygtbMHcA2C7rnRnZRn47W/xrwNMGvy57aGve8Hmhx953t5sV/TvHv/V6ClsMogj7/s1vf5/C9vozgl/Y7lu3AKGCHu8d3v63lwPf4YBwO/Gu7+A6li8/QzK6O627aA4yn6/07Xvt9Zh9BSyl+e53t4zcQ/IiYZ2ZvmNlVCW4z6ygppL/2t7G9H3gTOMrdy4HbAUtyDJsJmvhAcKoiXX9xbADe7+6D4x6F7t72DxkmlgO0m7eJ4EuldZs5YQwb4xfpJu7fEbRQxhF0jfwhblu/cfezCbqOIsCd3ayrI98GvgjEJ5TWvvHiuHk9SjgdiD+GMIigxbSJ4Av12Xbvc6m7xx8E7+o92gQMN7OSuHmHceB7nKiOtrOeoAUUH1+xuz/S0XJmdgTwU4L3dJi7DwaW8vb+3d3n3X6fKQOGkEB9wtbJte5+KPBlYLrFHVMbSJQUMk8ZwS/FWjM7DvhCP2zzSeBUM7vIgrNEvgqM6KL8/wL/aeE1EGZ2SOsBwx54BLjYzKosOJB+I8FxjTmJrsDd5wL7CLrYZrn7/jCe48xsipkVEPwqrwdiPYwPd18KPAb8S9zsLeHj0xZcCzKNuC+qXrrIghMMCgiOM/3d3TcTHNc43sw+aWZ54eN0izsg3038bwHzCD6rAjObCHyWoJ++R8KusJ0c2O31M+DLZnaaBUrDfaik47VQSvDFv53gt8fnCVoKrbYCleH+0JHfAdeY2Unhe3UnwXvVVasWgo19wsxaf+jsCeNo6W65bKSkkHluAK4i+IK8n6CLJKnCfvjLgLsJ/vGPBP4JNHayyN3AU8CzFpw19Q8O7EdOZJuLCOr5U4IvianAxeHxhZ74HXAeEH82SQHBGUQ7CL7AhwDfgrYLsl7rwfq/S5CoW+N24PMEff87CA48J5zIOvEbgmSwAziJ4MBya1fSBcCnCVpzWwi+CAt6sO7LgKPDZR8lOBZU3cs4vw08FHb9fNTdXyH41f9Tgm6c5WGsHXL31wkOer9KUJ9jOfC9+xuwAthqZls6WP4pgi7BP4bLH0ZwnCERZwBzw7O0/gB82QfoNTytZ2aIJMyC8903AZe6+99THY+I9B21FCQhZjbVzAaHzfLbCM7WeDXFYYlIH1NSkES9G1hN0JVzAfCR9qc4ikjmU/eRiIi0UUtBRETaZNxNqIYPH+5jx47t1bK1tbWUlHR2NlxmUB3SQ6bXIdPjB9Whp+bPn7/D3bs6lRzIwKQwduxY5s2b16tlq6urqaqq6tuA+pnqkB4yvQ6ZHj+oDj1lZmu7L6XuIxERiaOkICIibZQURESkjZKCiIi0UVIQEZE2SgoiItJGSUFERNpk3HUKqRR/S5D4u4N4R68fsFx82Y7XkWjZhqhT0xhNeHvxLzS1xKhvaqE5FiPa4kTDvztrG9m0p4H9DVFGDyniiOElHD+qnGAsHREZSAZMUvjZC6u56+la7K+z2n2BBlOdfqmmo2eeTvomzj5qGF877xhOG9ubIYhFJFMNmKRwUuUgLhyXx+GHHwaAxY1g2fqD+IDfxXG/kq3j2d2u44Cynfzqfnu5d66ro/WtWrWaI488oq18Zz/mrYP48yJGUX4ueREjNyeH3IiRm2MMLs5j9OBiSgtzeWt7La+s3sn9L6ziiumv8Pj1Z3P8qEEdb0REss6ASQpnHDGM+mPyqaoa333hNFYdW0/VuUcmbf0nVg7ixMpBXDqpkvPufp7b/vQmj153Fjk56koSGQh0oFk6NKQknxsvOJYF6/bw8uqdqQ5HRPqJkoJ06sOnjGZwcR4PzRmQQ9WKDEhKCtKpwrwIHzu1kqcXbWFvXXOqwxGRfqCkIF264PiRRGPOK2+pC0lkIFBSkC5NHDOYwrwc5qzelepQRKQfKClIl/Jzczh2ZDlLNu9LdSgi0g+UFKRb4yvKWLZ1/wFXUItIdkpqUjCzqWa2zMxWmtnNHbx+tZltN7OF4ePaZMYjvXPsyDJ21TaxvaYx1aGISJIl7eI1M4sA9wHvBzYAc81sprsvblf0YXe/PllxyMEbP7IMgGVb9nNIWWGKoxGRZEpmS+F0YKW7r3b3JmAGcEkStydJcmyYFJZu3p/iSEQk2SxZ/cRmdikw1d2vDaevBM6IbxWY2dXAncB2YDnwdXdf38G6pgHTACoqKibNmDGjVzHV1NRQWlraq2XTRarq8NXZdZwwLMLnTyo46HXpc0i9TI8fVIeemjJlynx3n9xduVTf++gJ4Hfu3mhmXwB+Cby3fSF3nw5MB5g8ebJXVVX1amPV1dX0dtl0kao6nLRqDrvrmqiqOueg16XPIfUyPX5QHZIlmd1HG4ExcdOV4bw27r7T3VuPXv4cmJTEeOQgjB9ZxoqtNbTEdAaSSDZLZlKYCxxtZuPMLB+4HJgZX8DMDo2bvBhYksR45CAcMaKUxmiMzXvrUx2KiCRR0rqP3D1qZtcDTwMR4AF3X2RmdwDz3H0m8BUzuxiIAruAq5MVjxycw4YWA7BuVx2VQ4pTHI2IJEtSjym4+yxgVrt5t8c9vwW4JZkxSN9oTQrrd9VB8oZzEJEU0xXNkpBDBxUSyTHW71L3kUg2U1KQhORGchhaks8OXdUsktWUFCRhw0ry2VHTlOowRCSJlBQkYcNLC9hZq5aCSDZTUpCEDSvNZ6daCiJZTUlBEjaspICdOqYgktWUFCRhw0rzqW1qob6pJdWhiEiSKClIwoaX5gPouIJIFlNSkIQNKwnukKrjCiLZS0lBEjZMLQWRrKekIAkbXhq0FHStgkj2UlKQhLW1FJQURLKWkoIkrDg/l6K8iE5LFcliSgrSI8NK89lZq5aCSLZSUpAeGVZaoJviiWQxJQXpkeElutWFSDZTUpAe0U3xRLKbkoL0SOtN8dw91aGISBIoKUiPDCstIBpz9tVHUx2KiCSBkoL0SOv9j3aoC0kkKykpSI/o/kci2U1JQXrk7aua1VIQyUZKCtIjw9q6j9RSEMlGSgrSI0OKg6SwS91HIllJSUF6JC+SQ0l+hP0NzakORUSSQElBeqy8KI99SgoiWUlJQXqsvDBP1ymIZCklBemx8qJctRREslRSk4KZTTWzZWa20sxu7qLcx8zMzWxyMuORvlFeqO4jkWyVtKRgZhHgPuBCYAJwhZlN6KBcGfBVYE6yYpG+VV6k7iORbJXMlsLpwEp3X+3uTcAM4JIOyv078H2gIYmxSB8qL1T3kUi2yk3iukcD6+OmNwBnxBcws1OBMe7+ZzO7sbMVmdk0YBpARUUF1dXVvQqopqam18umi3Sow65tTeyta2b27NmYWY+XT4c6HKxMr0Omxw+qQ7IkMyl0ycxygLuBq7sr6+7TgekAkydP9qqqql5ts7q6mt4umy7SoQ4rclbzxKolnHbWOZQW9HwXSoc6HKxMr0Omxw+qQ7Iks/toIzAmbroynNeqDDgBqDazNcC7gJk62Jz+youCRLCvXl1IItkmmUlhLnC0mY0zs3zgcmBm64vuvtfdh7v7WHcfC7wCXOzu85IYk/SB8sI8AB1XEMlCSUsK7h4FrgeeBpYAj7j7IjO7w8wuTtZ2JfnKi8KkoDOQRLJOUo8puPssYFa7ebd3UrYqmbFI32lrKaj7SCTr6Ipm6bG2YwrqPhLJOkoK0mNqKYhkLyUF6bGywtaWgo4piGQbJQXpsdxwTAW1FESyj5KC9IrGVBDJTkoK0isaU0EkOykpSK9oTAWR7KSkIL2iMRVEspOSgvSKxlQQyU5KCtIrGlNBJDt1mRTMLGJmv+qvYCRzBC2FZtw91aGISB/qMim4ewtwhJnl9VM8kiHKC/OIOdQ2taQ6FBHpQ4ncEG8V8HczexyobZ3p7vckLSpJe/FjKvRmoB0RSU+J/DevCx/F4UPkgDEVRlGU4mhEpK90mxTc/TYAMysKp+uTHZSkP42pIJKduj37yMwmmNlcYAWwwszmmNlxyQ9N0pnulCqSnRI5JXU6cKu7V7p7JfAt4GfJDUvSncZUEMlOiSSFMnf/W+uEuz8DlCUvJMkEaimIZKdEksIaM7vFzCrDx83AmiTHJWlOYyqIZKdEksLngDEEYy3/GagM58kApjEVRLJTl2cfmVkEuNHdv9RP8UgG0ZgKItknkSuap/RTLJJhNKaCSPZJ5OK1+Wb2B+D3HHhF88ykRSUZQWMqiGSfRJJCGUEy+EDcPAeUFAa48sI8tu5vSHUYItKHEjmmMFf3OZKOlBflsWJbTarDEJE+lMgxhU/3UyySYTSmgkj2SaT76EUz+x/gYQ48pvB60qKSjBA/poKZpTocEekDiSSF08K/k+LmOXBu34cjmSR+TAXdPlskOyRyl9RzertyM5sK/AiIAD9397vavX4d8GWgBagBprn74t5uT/qXxlQQyT6J3CV1hJndb2ZPhtMTzOzqBJaLAPcBFwITgCvMbEK7Yg+5+4nuPhH4AXB3TysgqRM/poKIZIdEbnPxC+B5gltdQHAL7RsSWO50YKW7r3b3JmAGcEl8AXffFzdZQtAtJRlCYyqIZJ9E2vyHuPtDZnYjgLs3m1ksgeVGA+vjpjcAZ7QvZGZfBr4B5APv7WhFZjYNmAZQUVFBdXV1Apt/p5qaml4vmy7SqQ5r9gbjM780dwF1axPvPkqnOvRWptch0+MH1SFp3L3LB1ANDAUWhNOnAX9PYLlLCY4jtE5fCdzbRflPAr/sbr2TJk3y3po9e3avl00X6VSHNTtq/PB/fdIfm7++R8ulUx16K9PrkOnxu6sOPQXM826+X909oZbCN4EngCPM7HmCFsClCSy3kbe7nCC4u+rGLsrPAH6awHolTWhMBZHsk8jZR/PMbApwHGDAYg+OEXRnLnC0mY0jSAaXE7QG2pjZ0e6+Ipz8IMHxCskQGlNBJPsk1BEcJoHXerJid4+a2fXA0wSnpD7g7ovM7A6CZsxM4HozOw9oBnYDV/Uoekkpjakgkn2SenK5u88iGJwnft7tcc+/msztS/JpTAWR7JLIKakindKYCiLZpduWgpmd1MHsvcB6d0/k1FTJYhpTQSS7JNJ99H/ARGARwYHm44DFQJmZTXP3Z5MYn6Q5jakgkl0S6T5aA0xy94nufjLBjfGWAxcA/y+JsUkGCO6Uqu4jkWyRSFI4zuNuk+3ubwAT3H1l8sKSTKExFUSySyLdR0vN7McEF5cBXBbOKwD0E3GA05gKItklkZbCZwjuW3Rz+NhEcD1BFHhf8kKTTBA/poKIZL5ErmiuA74fPtrb2+cRSUbRmAoi2SWRU1LfBXwbODy+vLsfk8S4JEPEj6kwiqIURyMiByuRn3YPAjcB8wlGSBNpozEVRLJLIklhn7s/kfRIJCPpTqki2SWRpPCcmd0J/AFobJ0Zf5qqDFxtxxR0WqpIVkgkKby73V8Ihs08t+/DkUyjloJIdknk7KNz+iMQyUwaU0Eku3SaFMzsCnf/nZl9paPX3f2e5IUlmUJjKohkl65aCkPCvyP6IxDJXBpTQSR7dJoU3P0n4d/b+i8cyUQaU0EkeyRy8dpw4HPAWA68eG1a8sKSTKIxFUSyRyJnHz0OvAK8iC5ekw5oTAWR7JFIUihx9xuSHolkrPKiPJZt3Z/qMESkDyRyl9S/mNn5SY9EMtbQknx21zalOgwR6QOJJIXrgKfMrMbMdpnZbjPblezAJHMMLcmntqmFhmb1LopkukS6j4YnPQrJaMNL8wHYWdvE6MG6U6pIJuvq4rWj3X0FcHwnRXTvIwFgaEkBADtrGpUURDJcVy2Fm4FrgPs6eE33PpI2w+JaCiKS2bq6eO2a8K/ufSRdGlEatBS272vspqSIpLuExk80s/HABKCwdZ67P5SsoCSzVJQXYgab9tanOhQROUiJXNH8b8D5wHjgaeACggvZlBQEgPzcHA4pK2DjbiUFkUyXyCmplwFTgM3ufiVwMlCSyMrNbKqZLTOzlWZ2cwevf8PMFpvZ62b2rJkd3qPoJW2MGlykloJIFkgkKdS7ewsQNbMyYAvQ7Ze3mUUIDlJfSND1dIWZTWhX7J/AZHc/CXgU+EFPgpf0UTmkmDU76nhozjp+9sLqVIcjIr2USFL4p5kNBh4A5gGvho/unA6sdPfV7t4EzAAuiS/g7rPdvS6cfAWoTDhySSsnji5n4556bv3jG/zHrCWpDkdEesncvfMXzQwY6e6bw+mjgHJ3X9Dtis0uBaa6+7Xh9JXAGe5+fSfl7wW2uPv3OnhtGjANoKKiYtKMGTO6rVhHampqKC0t7dWy6SJd67BsVwt3vvr2TfF+MbXzHsZ0rUNPZHodMj1+UB16asqUKfPdfXJ35bo80OzubmZ/A04Ip1f2UXwHMLNPA5OB93QSx3RgOsDkyZO9qqqqV9uprq6mt8umi3StwzkxZ2HdAv7y5hYAJp/5bkoLOt690rUOPZHpdcj0+EF1SJZEuo8WmtkpvVj3RmBM3HRlOO8AZnYe8C3gYnfXie4ZKpJj/PTTk7j7EycDsG2fbqUtkok6TQpm1voz7xRgbngW0QIz+6eZddt9BMwFjjazcWaWD1wOzGy3jVOA+wkSwrbeVUHSSUV5cCnLFiUFkYzUVffRq8CpwMW9WbG7R83seoJrGyLAA+6+yMzuAOa5+0zgh0Ap8Pvg8AXr3L1X25P0MHZ4cCxh9fZazjpS91IUyTRdJQUDcPdVvV25u88CZrWbd3vc8/N6u25JT6MGFVJWmMtDc9ZxwuhBTBwzONUhiUgPdJUURpjZNzp70d3vTkI8kuHMjGMrypi3djcfvu8l1tz1wVSHJCI90NWB5ghB105ZJw+RDl06SZebiGSqrloKm939jn6LRLLGZaeN4dW3dvGHf25kT10Tg4vzUx2SiCSoq5aC9VsUklXMjAtPPBSANTvruiktIumkq6Twvn6LQrLO2GHFAKzZUZviSESkJzpNCu6+qz8DkewyZmgxZvC1hxdy++NvpjocEUlQIlc0i/RYYV6EkvzgkNWvXl6b4mhEJFFKCpI0n3v3OABMR6dEMoaSgiTNN95/DDdNPRZ3eGz+hlSHIyIJUFKQpBo7LLjtxbf+9EaKIxGRRHQ7RrPIwXj/hAqGlxZQXqhdTSQTqKUgSZUXyeHDE0exaW89XQ3oJCLpQUlBkm7U4CIammPsqWtOdSgi0g0lBUm6MUPDC9l26kI2kXSnpCBJd0xFMAbt8q37UxyJiHRHSUGSbsyQYoryIizetC/VoYhIN3RKiCRdTo5x9lHD+eXLa1m4YS9fGq8DziLpSi0F6Rc3TT0WgNfW7+FbL9YTbYmlOCIR6YiSgvSLYyrKuOeKUwDYUe889Oq6FEckIh1RUpB+c/HJo9paDLc/vogtexsA+P289XzuF3NTGZqIhJQUpF99qeooCiPB8zlv7QTgxkdf57ml22iKqktJJNWUFKTf/eS8YoaV5HPvcytpib190HlXbVMKoxIRUFKQFMgx45sXHMuKbTX84OmlbfN31DSmMCoRASUFSZGzjxwOwP3Pr26bp6QgknpKCpISY4YW8eGJozhyRAnfuWgCADtr1H0kkmq6eE1Swsz4n8uDU1RrG6N854nFbN3fkOKoREQtBUm5koJcBhfnsXF3PQ+8+BY3PPJaqkMSGbCUFCQtjBlSzPrd9dzx5GIeW7BBYy+IpEhSk4KZTTWzZWa20sxu7uD1c81sgZlFzezSZMYi6W3M0CKWbXn7hnnffWJxCqMRGbiSlhTMLALcB1wITACuMLMJ7YqtA64GHkpWHJIZDhtawtZ9b5999It/rEldMCIDWDJbCqcDK919tbs3ATOAS+ILuPsad38d0KWsA9z4kWXvmNcYbUlBJCIDWzLPPhoNrI+b3gCc0ZsVmdk0YBpARUUF1dXVvQqopqam18umi2ytw/797/xdMPOvzzOiOD0Pe2X655Dp8YPqkCwZcUqqu08HpgNMnjzZq6qqerWe6upqertsusjWOkRbYrzR+AafOXMsO2oaufrBuYybMJHJY4emJshuZPrnkOnxg+qQLMlMChuBMXHTleE8kXfIjeTwg0tPBmDJ5uCA86a9um5BpL8ls20+FzjazMaZWT5wOTAziduTLHHEiBLyc3N4bf2eVIciMuAkLSm4exS4HngaWAI84u6LzOwOM7sYwMxOM7MNwMeB+81sUbLikcxRkBth4pjBzF2zK9WhiAw4ST2m4O6zgFnt5t0e93wuQbeSyAFOHzuUe2ev5N7nVnBIWSEFeTlcMnF0qsMSyXoZcaBZBp7Txg2F2fBff13eNk9JQST50vN8PxnwzjpyGF+sOvKAeXVNUXbWNHLnrCW6hkEkSZQUJC3lRXL416njuX7KUZgF85ZvreE/Zi3h/hdWU71se2oDFMlSSgqS1r55wbH84+b3AjB/7W7W7qwDYNOe+lSGJZK1dExB0t6hg4oYM7SIh+euY02YFJZvrUlxVCLZSS0FyQifmDSG5VtraIrGqBxSxPKt+1MdkkhWUktBMsJ1VUdS0xilcmgxy7bs4/GFm3B3rPWAg4j0CSUFyQh5kRxu+cBxAPzq5TXsb4iyZmcd44aXpDYwkSyj7iPJOO87roL8SA5T/quabz/+JtGWGLGYRmoT6QtKCpJxRg8u4udXTQbgly+v5eJ7X+Kob81iXXgQWkR6T0lBMtK5x4xgyR1TOfqQUhZv3kfM4dwfzmZvfXOqQxPJaEoKkrGK8iP89toz+FLclc8vr9qRwohEMp+SgmS0Q8oLuWnqeG77UDD893W/WcBGXdgm0ms6+0iywjXvHseyLft4ZN4Gzr7rOW684FhGlhdyxIgSVmyr4cITRlJWmJfqMEXSnpKCZI3bLzqeR+ZtAOCHTy874LWbHn2dNXd9MBVhiWQUdR9J1igtyOWZb5zb6bULG3br7CSR7igpSFY56pAynrvhPaz6zw/wqTMO42OnVjK4OOg2eurNLSmOTiT9qftIso6ZETH4j4+c2Dbv8ukvc/fflrN2Zx0rtu3nk2cczkUnHYqZsa+hmSde28QVpx1GTo5umyEDm5KCDAg/vPRkvv7wQn79yloAXlm9iw276/hS1VF84+GFPLNkG+NHljHp8KEj+J8iAAAOKklEQVQpjlQktZQUZEAYM7SYR794FjtrGrnqwVd5c+M+fvDUMp56cwuvb9gLwNIt+5UUZMDTMQUZUIaVFvDodWcx/cpJAG0JAWDB2j2pCkskbailIANOYV6E848fyWNfPIs7Zy3hXUcMY1ddEw/NWcezS7dy3XuO5OqzxlKYF0l1qCL9TklBBqxJhw/h0S+eBcDe+mZ21jTy/PLt3PWXpTw0Zx2fP2ccHz21kpKCXI3dIAOGkoIIMKgoj/uvDO68+tLKHXz/qaXc9vgivv/UMo4cUcLGPfXc9qEJXDJxdKfrWL29hvzcHCqHFPdX2CJ9TklBpJ2zjxrO418+mwXr9vDLf6zhidc34Q5fnbGQhev38LFTK2kJx29Yv6uOGx55jQtPHMl3n1gMwIOfPY0pxx7Str7mlhhb9jYwZqiShaQ/JQWRDpgZkw4fwqTDh/BvHzyOhev38O9/XsyDL63hwZfWUJQLH9z5Gm9u3MvSLft5dc2utmU/++BcPnPm4dxxyQkAfO/Jxfzy5bXM/dZ5jCgrSFWVRBKipCDSjUPKCzn/+JGcf/xIlm7Zx98WbeW511bxlzc20xxzxgwtYv2uesaPLGPplv0A/OrltVQdO4K/vLGF388P7sc0e+k2Pj65khdX7uDvK3Zwy4XjdZxC0o6SgkgPjB9ZzviR5ZwY2cgZZ51DizulBW//GzW3xLhv9krufW4ln/vFvAOWvemx1/nLm5uZvWw7ADtrmvjoqaN5dP4Grj5rLCePGUxTNEZTS4zcHCM/kqMrrKXfJTUpmNlU4EdABPi5u9/V7vUC4FfAJGAncJm7r0lmTCJ9pSj/naes5kVy+Np5x3DRyaP4x6qdDCrKI8dgRGkBn3ngVWYv205ZQS77G6M8tmADjy0IWhFPvr6Jw4eVsKOmkT11wehxx1SUctaRw8kxY0dNI6++tYuPT65keGkBLTHnkPICCnIjDC3JB5z6phiN0RbyIjmcVDmIQUV5/HXxVp5fvp0LTxjJOUePaIszGnNiMVfSkXdIWlIwswhwH/B+YAMw18xmuvviuGLXALvd/Sgzuxz4PnBZsmIS6S9HjijlyBGlB8xb9r0L2ba/gcFF+dQ0Rnlh+XZ21DRy2tihPDJvPbtqmxg9uIjddU1s2dvA1n2NPLZgA+5Q0xgF4MfPrUw4hpL8CLVNLQA8NGcdowYVAjC8rIAVW+o4dMHzjBxUyKjBReypa2J4aQF765sZUVbAok37OHxoMcUFEYryIuyua+boQ0pZu6uOccOCs7EqhwSxRnJyGFKcR2M0RnF+hPLCPOa8tZNV22qZMv4Q9jc00xiNcUxFKYOL89m6r4Fxw0soLchlT10zy7bu57hDyynOj1DTGCXa4qzdWcsphw2mvDCPssI86ptbcHcK8yI4kB/JobbZqW2MEo05LTEnGouxYXc9lYOL2F3XjOMYRl7EKMxrTZ4EMZtRXpTH/oYoBXk5RFucffXN1DZFGTe8hBwzYu40R51IxGhsbmFPfTMNzS2MG15CJMfYuLue2sYWRg0upKwwj3W76ojGYowaXERpfi4ObNpT35a8Ozutub6pBTO6vC5mb30zpQW5RPohiSezpXA6sNLdVwOY2QzgEiA+KVwCfCd8/ihwr5mZu3sS4xJJmUPKgi/mobn5fPiUt09vPXnM4G6XbYy2UNvYQjQWo6YhSkNzjPrmFvY1BC2LHDNyc4y6phaWbN7Hnrpmxg4vpqK8kAXrdrN+Vx0NzTGaW2IMKTRa3Fmzo5blW2vIjxj1zS0U5Eaoa4qyryHK/LW7ASjIzaG0IJdH52+gMC+HhuZYt7EW5uUwpDif7z+1lEiO4e7EkvFf/ezTSVhp34rkGMV5EWqbouRGcijIzaGxOUZODpjHiP4tqENRXoRozCnIzQm//C1MJLCjpomK8gJu/cBxXZ4W3RcsWd+/ZnYpMNXdrw2nrwTOcPfr48q8GZbZEE6vCsvsaLeuacA0gIqKikkzZszoVUw1NTWUlpZ2XzCNqQ7pIdPrkEj8MXfcofXH6Z5GZ1CBBX/zjdooRCx47GtyDGiOBV1TI4pzKIzAplpnWKGRH4G39saoj8KIImNzbYwWh6YWOGpwDqv2xKiNBusozDUOK8vhrb1BK6c5BgXhj+ho+HUVjUFtfSO5+flEzMgJ4yjONfY1Obk57esCDS1BYsrLMRwnGoOiiFHfEtRzSKFREDG21sVwBzPINYgBhRGjJC+4++7WuhhG8L4U5hoNLU5TCwwpMDDY3+Q0B6FTmm/sbnAaW5yiXCPm0Bxz8nKC5w2NTRQW5GFmNLcECaCpBZzgHkQ5Bi0evActMefcyjyOH967K+2nTJky390nd1cuIw40u/t0YDrA5MmTvaqqqlfrqa6uprfLpgvVIT1keh0yPX5QHZIlmTfE2wiMiZuuDOd1WMbMcoFBBAecRUQkBZKZFOYCR5vZODPLBy4HZrYrMxO4Knx+KfCcjieIiKRO0rqP3D1qZtcDTxOckvqAuy8yszuAee4+E/g/4NdmthLYRZA4REQkRZJ6TMHdZwGz2s27Pe55A/DxZMYgIiKJ0yA7IiLSRklBRETaKCmIiEgbJQUREWmTtCuak8XMtgNre7n4cGBHt6XSm+qQHjK9DpkeP6gOPXW4u4/orlDGJYWDYWbzErnMO52pDukh0+uQ6fGD6pAs6j4SEZE2SgoiItJmoCWF6akOoA+oDukh0+uQ6fGD6pAUA+qYgoiIdG2gtRRERKQLSgoiItJmwCQFM5tqZsvMbKWZ3ZzqeDpjZg+Y2bZwVLrWeUPN7G9mtiL8OyScb2Z2T1in183s1NRF3hbrGDObbWaLzWyRmX01nJ9JdSg0s1fN7LWwDt8N548zszlhrA+Ht4THzArC6ZXh62NTGX8rM4uY2T/N7MlwOqPiBzCzNWb2hpktNLN54bxM2pcGm9mjZrbUzJaY2ZnpHv+ASApmFgHuAy4EJgBXmNmE1EbVqV8AU9vNuxl41t2PBp4NpyGoz9HhYxrw036KsStR4AZ3nwC8C/hy+F5nUh0agfe6+8nARGCqmb0L+D7w3+5+FLAbuCYsfw2wO5z/32G5dPBVYEncdKbF32qKu0+MO58/k/alHwFPuft44GSCzyO943f3rH8AZwJPx03fAtyS6ri6iHcs8Gbc9DLg0PD5ocCy8Pn9wBUdlUuXB/A48P5MrQNQDCwAziC48jS3/T5FMGbImeHz3LCcpTjuSoIvnPcCTwKWSfHH1WMNMLzdvIzYlwhGknyr/XuZ7vEPiJYCMBpYHze9IZyXKSrcfXP4fAtQET5P63qF3RCnAHPIsDqEXS8LgW3A34BVwB53j4ZF4uNsq0P4+l5gWP9G/A7/A9xEMO48BPFkUvytHPirmc03s2nhvEzZl8YB24EHw268n5tZCWke/0BJClnDg58QaX8esZmVAo8BX3P3ffGvZUId3L3F3ScS/OI+HRif4pASZmYfAra5+/xUx9IH3u3upxJ0rXzZzM6NfzHN96Vc4FTgp+5+ClDL211FQHrGP1CSwkZgTNx0ZTgvU2w1s0MBwr/bwvlpWS8zyyNICL919z+EszOqDq3cfQ8wm6C7ZbCZtY5WGB9nWx3C1wcBO/s51HhnAxeb2RpgBkEX0o/InPjbuPvG8O824I8ECTpT9qUNwAZ3nxNOP0qQJNI6/oGSFOYCR4dnX+QTjAU9M8Ux9cRM4Krw+VUE/fSt8z8TnrXwLmBvXLM0JczMCMbeXuLud8e9lEl1GGFmg8PnRQTHRJYQJIdLw2Lt69Bat0uB58JfgCnh7re4e6W7jyXY159z90+RIfG3MrMSMytrfQ6cD7xJhuxL7r4FWG9mx4az3gcsJt3jT9VBmP5+AB8AlhP0DX8r1fF0EefvgM1AM8EvjWsI+nefBVYAzwBDw7JGcFbVKuANYHIaxP9ugubw68DC8PGBDKvDScA/wzq8Cdwezj8CeBVYCfweKAjnF4bTK8PXj0h1HeLqUgU8mYnxh/G+Fj4Wtf7fZti+NBGYF+5LfwKGpHv8us2FiIi0GSjdRyIikgAlBRERaaOkICIibZQURESkjZKCiIi0UVKQAcfMasK/Y83sk3287lvbTf+jL9cvkmxKCjKQjQV6lBTirgjuzAFJwd3P6mFMIimlpCAD2V3AOeG9+r8e3gTvh2Y2N7yf/RcAzKzKzP5uZjMJrkjFzP4U3qRtUeuN2szsLqAoXN9vw3mtrRIL1/1mOD7AZXHrro675/5vw6vCMbO7LBiX4nUz+69+f3dkQOruV49INrsZ+Ka7fwgg/HLf6+6nmVkB8JKZ/TUseypwgru/FU5/zt13hbfBmGtmj7n7zWZ2vQc30mvvowRXt54MDA+XeSF87RTgeGAT8BJwtpktAT4CjHd3b73thkiyqaUg8rbzCe49s5Dgdt/DCAY8AXg1LiEAfMXMXgNeIbiJ2dF07d3A7zy4++pW4HngtLh1b3D3GMFtQcYS3L66Afg/M/soUHfQtRNJgJKCyNsM+BcPRvma6O7j3L21pVDbVsisCjiPYGCakwnuk1R4ENttjHveQjAQTpTgjqCPAh8CnjqI9YskTElBBrL9QFnc9NPAF8Nbf2Nmx4R352xvEMHwlXVmNp5g2NFWza3Lt/N34LLwuMUI4FyCm891KByPYpC7zwK+TtDtJJJ0OqYgA9nrQEvYDfQLgjEHxgILwoO924EPd7DcU8B1Yb//MoIupFbTgdfNbIEHt6tu9UeCMRleI7iL7E3uviVMKh0pAx43s0KCFsw3eldFkZ7RXVJFRKSNuo9ERKSNkoKIiLRRUhARkTZKCiIi0kZJQURE2igpiIhIGyUFERFp8/8BP9GAmF3f67IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist)\n",
    "plt.grid()\n",
    "plt.title(\"Training error vs. Number of iterations\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Training error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Multi Layer Perceptron (10 points)\n",
    "\n",
    "Multi Layer Perceptron (MLP) is a fully connected deep (more than one hidden layer) network. In this part of the coursework we will implement a 2 hidden layers MLP with Recified Linear Unit (ReLU) activations. We will train the model via ADAM optimizer over a cross-entropy loss function.\n",
    "\n",
    "First of all, we will convert our label vectors to matrices via one-hot encoding (e.g. $y=2$ would become $[0,0,1,0,0,0,0,0,0,0]$). This can be simply done using commands below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define a class MLP. It is initialized via:  \n",
    "\n",
    "+ x_train: The training matrix.\n",
    "+ y_train: One hot encoding of the corresponding labels.\n",
    "+ lr: Learning rate used for ADAM optimizer\n",
    "+ nb_epochs: Number of epochs to use\n",
    "+ batch_size: The number of data point in each mini-batch\n",
    "+ output_dir: The directory where model parameters and tensorboard event files will be stored.\n",
    "\n",
    "We also define the methods: \n",
    "* 'create_model' which will desribe a neural network architecture of the form $[784, 1000, 1000, 10]$, each integer representing the number of neurons in a given layer while the length of the vector defines the number of layers accordingly. \n",
    "* 'compute_loss' which given the output of 'create_model' will calculate the cross-entropy loss of the mini-batches.\n",
    "* 'train' where we initiate a tensorflow session and perform the training iterations. \n",
    "* 'test' where we load our trained model and perform inference on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 3.1\n",
    "- Complete the method 'create_model' in order to implement a network of the shape $[784, 1000, 1000, 10]$, use ReLU as the non linear activation for hidden layers.\n",
    "\n",
    "   The function 'create_model' to complete defines the class variables: \n",
    "\n",
    "   + self.logits $\\in \\mathbb{R^{10}}$ containing the output __<font color='red'>without activation of the MLP.</font>__\n",
    "   + self.preds $\\in \\mathbb{R^{10}}$ containing posterior probabilities.\n",
    "\n",
    "- Using self.logits complete the method 'compute_loss' that takes the labels and the predicted logits to return the corresponfing cross-entropy loss. \n",
    "\n",
    "Hints: \n",
    "- You may use tf.layers.dense to implement a fully connected layer. \n",
    "- To obtain the probabilities you must normalize your outputs in a way that their sum is equal to one using a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, x_train, y_train, output_dir, lr=0.001, nb_epochs=10, batch_size=50):\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.nb_images, self.nb_features = x_train.shape\n",
    "        self.nb_iterations = self.nb_images // batch_size\n",
    "        self.output_dir = output_dir\n",
    "        self.im = tf.placeholder(tf.float32, [None, 784])\n",
    "        self.labels = tf.placeholder(tf.float32, [None, 10])\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def create_model(self):\n",
    "        with tf.variable_scope('MLP', reuse=tf.AUTO_REUSE):\n",
    "        ######### Complete the function ######### \n",
    "            self.logits = \n",
    "            self.preds =\n",
    "        #########################################\n",
    "    def compute_loss(self):\n",
    "        with tf.variable_scope('loss'):\n",
    "            ######### Complete the function ######### \n",
    "            self.loss = \n",
    "            #########################################\n",
    "            self.loss_summ = tf.summary.scalar(\"softmax_loss\", self.loss)\n",
    "            \n",
    "    def optimizer(self):\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.5)\n",
    "            self.model_vars = tf.trainable_variables()\n",
    "            self.trainer = optimizer.minimize(self.loss, var_list=self.model_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we defined our model, our loss and its optimizer. we can instantate the MLP class, initiate our variables, and start the tensorflow session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(x_train, y_train, './MLP_logdir/', 0.001, 2, 10)\n",
    "model.create_model()\n",
    "model.compute_loss()\n",
    "model.optimizer()\n",
    "init = (tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "summary =tf.Summary()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "writer = tf.summary.FileWriter(model.output_dir)\n",
    "writer.add_graph(sess.graph)\n",
    "if not os.path.exists(model.output_dir):\n",
    "    os.makedirs(model.output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start training. We loop over the training data points and we feed them to the session in mini-batches form. we repeat this process several times (for several epochs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(model.nb_epochs):\n",
    "    randomize = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(randomize)\n",
    "    x_in = model.x_train[randomize,:]\n",
    "    y_in = model.y_train[randomize,:]\n",
    "    for i in range(model.nb_iterations):\n",
    "        input_x_train = x_in[i*model.batch_size: (i+1)*model.batch_size]\n",
    "        input_y_train = y_in[i*model.batch_size: (i+1)*model.batch_size]\n",
    "        _ , preds, loss, loss_summ = sess.run([model.trainer, model.preds, model.loss, model.loss_summ], \n",
    "                                 feed_dict={model.im: input_x_train, \n",
    "                                            model.labels: input_y_train})\n",
    "        y_preds = np.argmax(preds, axis=1)\n",
    "        y_real = np.argmax(input_y_train, axis=1)\n",
    "        acc_train = np.mean((y_preds==y_real)*1)\n",
    "        print('Epoch %d, Iteration %d, loss %.3f, batch accuracy %.3f' %(epoch, i, loss, acc_train))\n",
    "        writer.add_summary(loss_summ, epoch * model.nb_iterations + i)\n",
    "    saver.save(sess, model.output_dir, global_step=epoch)  \n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During and after training visualize the training and the created graph via tensorboard. Tensorboard is accessible via the command \"tensorboard --logdir=#yourlogdir#\". Check out https://www.tensorflow.org/guide/summaries_and_tensorboard for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we loop over the test dataset and get the test accuracy via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_test = 20\n",
    "nb_test_points = x_test.shape[0] \n",
    "nb_iterations = nb_test_points//batch_size_test\n",
    "preds = []\n",
    "for i in range(nb_iterations):\n",
    "    input_x_test = x_test[i*batch_size_test: (i+1)*batch_size_test]\n",
    "    preds_test = sess.run(model.preds, \n",
    "                             feed_dict={model.im: input_x_test})\n",
    "    preds.append(np.argmax(preds_test, axis=1))\n",
    "    if np.mod(nb_test_points, batch_size_test) !=0:\n",
    "        input_x_test = x_test[i*batch_size_test: -1]\n",
    "        preds_test = sess.run(model.preds, \n",
    "                             feed_dict={model.im: input_x_test})\n",
    "        preds.append(np.argmax(preds, axis=1))\n",
    "all_preds = np.concatenate(preds, axis =0)\n",
    "y_real = np.argmax(y_test, axis=1)\n",
    "acc_test = np.mean((all_preds==y_real)*1)\n",
    "print('Test accuracy achieved: %.3f' %acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Include a figure to visualize your training & testing(see below) performances during iterations and discuss your observations in your report.\n",
    "\n",
    "\n",
    "We now close the tensorflow session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 3.2\n",
    "Using a similar format as before, discuss how the number of layers affects the classification accuracy. \n",
    "- Train four different networks with more hidden layers for example 3,4,5 and 7 hidden layers (choice is yours here to make a good conclusion). Choose an appropriate width i.e. number of neurons per layer so to achieve good accuracy and feasible training time. How many paramters (weights/biases) do these models have? How do they compare to the former MLP you implemented? \n",
    "- Compare the classification accuracies of these networks with the previous MLP.\n",
    "\n",
    "\n",
    "- Plot a graph showing the accuracy vs. depth v.s. complexity (number of paramters) of the all five MLPs with different depths/widths. Additionally report the results in a Table. Discuss the results and provide conclusion. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convolutional Neural Network (10 points)\n",
    "Now that we are more familiar with the MLP algorithm, it is time to see how it compares with a Convolutional Neural Network (CNN). CNNs leverage dependencies between neighbouring pixels, making them more efficient and light weight compared to their fully connected counter part. In this section we will implement a Class CNN similar to the one we defined before for MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 4.1\n",
    "Complete the function 'create_model' of the class CNN above. Implement a CNN of the shape [32, 64, 128]; where 32, 64, 128 represent the number of convolutional filters for each hidden layer. We will use a kernel size of size $4\\times4$. \n",
    "\n",
    "Use a stride of 1 in the first convolutional layer, followed by a stride of 2 for the following layers (a stride of two help downsampling without requiring the use of pooling layers). Vectorize the obtained output using tf.layer.flatten, and end the model with a fully connected layer of 10 neurons. Use ReLU as the non linear activation for the hidden layers.\n",
    "\n",
    "   The function 'create_model' to complete defines the class variables: \n",
    "\n",
    "   + self.logits $\\in \\mathbb{R^{10}}$ containing the output without activation of the last __<font color='red'>fully connected layer</font>.__ \n",
    "\n",
    "   + self.preds $\\in \\mathbb{R^{10}}$ containing posterior probabilities.\n",
    "\n",
    "- Using self.logits complete the method 'compute_loss' that takes the labels and the predicted logits to return the corresponfing cross-entropy loss. \n",
    "\n",
    "Hints: \n",
    "+ You may use tf.layers.conv2d to implement a convolutional layer. \n",
    "+ To obtain the probabilities you must normalize your outputs in a way that their sum is equal to one using a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, x_train, y_train, output_dir, lr=0.001, nb_epochs=10, batch_size=50):\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.nb_images, self.edge, _ = x_train.shape\n",
    "        self.nb_iterations = self.nb_images // batch_size\n",
    "        self.output_dir = output_dir\n",
    "        self.im = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "        self.labels = tf.placeholder(tf.float32, [None, 10])\n",
    "        self.x_train = x_train[:,:,:, np.newaxis]\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def create_model(self):\n",
    "        with tf.variable_scope('CNN', reuse=tf.AUTO_REUSE):\n",
    "            ######### Complete the function ######### \n",
    "            self.logits = \n",
    "            self.preds =\n",
    "            #########################################\n",
    "    \n",
    "    def compute_loss(self):\n",
    "        with tf.variable_scope('loss'):\n",
    "            ######### Complete the function ######### \n",
    "            self.loss\n",
    "            #########################################\n",
    "            self.loss_summ = tf.summary.scalar(\"softmax_loss\", self.loss)\n",
    "    def optimizer(self):\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.5)\n",
    "            self.model_vars = tf.trainable_variables()\n",
    "            self.trainer = optimizer.minimize(self.loss, var_list=self.model_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs leverage dependencies between neighbouring pixels, however this information is partially lost when we vectorized our data. For training CNNs we will need to recover our initial shape of $N \\times 28 \\times 28$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge = int(np.sqrt(nb_features))\n",
    "x_train.resize([n_train, edge, edge])\n",
    "x_test.resize([n_test, edge, edge])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate our CNN class, start the corresponging tensorflow session and initiate the trainable variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(x_train, y_train, './CNN_logdir/', 0.001, 2, 10)\n",
    "model.create_model()\n",
    "model.compute_loss()\n",
    "model.optimizer()\n",
    "init = (tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "summary =tf.Summary()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "writer = tf.summary.FileWriter(model.output_dir)\n",
    "writer.add_graph(sess.graph)\n",
    "if not os.path.exists(model.output_dir):\n",
    "    os.makedirs(model.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(model.nb_epochs):\n",
    "    randomize = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(randomize)\n",
    "    x_in = model.x_train[randomize,:]\n",
    "    y_in = model.y_train[randomize,:]\n",
    "    for i in range(model.nb_iterations):\n",
    "        input_x_train = x_in[i*model.batch_size: (i+1)*model.batch_size]\n",
    "        input_y_train = y_in[i*model.batch_size: (i+1)*model.batch_size]\n",
    "        _ , preds, loss, loss_summ = sess.run([model.trainer, model.preds, model.loss, model.loss_summ], \n",
    "                                 feed_dict={model.im: input_x_train, \n",
    "                                            model.labels: input_y_train})\n",
    "        y_preds = np.argmax(preds, axis=1)\n",
    "        y_real = np.argmax(input_y_train, axis=1)\n",
    "        acc_train = np.mean((y_preds==y_real)*1)\n",
    "        print('Epoch %d, Iteration %d, loss %.3f, batch accuracy %.3f' %(epoch, i, loss, acc_train))\n",
    "        writer.add_summary(loss_summ, epoch * model.nb_iterations + i)\n",
    "    saver.save(sess, model.output_dir, global_step=epoch)  \n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we loop over the test dataset and compute the corresponding classification accuracy. \n",
    "\n",
    "- Similar to Task 3.1, include a figure to visualize your training & testing(see below) performances during iterations and discuss your observations in your report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_test = 20\n",
    "nb_test_points = x_test.shape[0] \n",
    "nb_iterations = nb_test_points//batch_size_test\n",
    "preds = []\n",
    "for i in range(nb_iterations):\n",
    "    input_x_test = x_test[i*batch_size_test: (i+1)*batch_size_test]\n",
    "    input_x_test = input_x_test[:, :, :,np.newaxis]\n",
    "    preds_test = sess.run(model.preds, \n",
    "                             feed_dict={model.im: input_x_test})\n",
    "    preds.append(np.argmax(preds_test, axis=1))\n",
    "    if np.mod(nb_test_points, batch_size_test) !=0:\n",
    "        input_x_test = x_test[i*batch_size_test: -1]\n",
    "        preds_test = sess.run(model.preds, \n",
    "                             feed_dict={model.im: input_x_test})\n",
    "        preds.append(np.argmax(preds, axis=1))\n",
    "all_preds = np.concatenate(preds, axis =0)\n",
    "y_real = np.argmax(y_test, axis=1)\n",
    "acc_test = np.mean((all_preds==y_real)*1)\n",
    "print('Test accuracy achieved: %.3f' %acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 4.2\n",
    "Similar to task 3.2, discuss \n",
    "- Plot a graph showing complexity (number of paramters) vs. accuracy vs. depth of networks. For this part you need to train four additional CNNs of different depths/widths (again, your choice) and report the results in a Table. Discuss the results and provide conclusion.\n",
    "\n",
    "\n",
    "- In addition discuss and analyze the differences in term of performance, number of model parameters (i.e. weights/biases) and training/testing times between CNNs and MLPs. Provide a concusion. (For these discussions you should compare your results in Table 4.2 to Table 3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualizing learned filters and activations (6 points)\n",
    "It is sometimes useful to visualize what kind of filters a CNN have learned. One way to do so is to plot each filter of size [kernel_size $\\times$ kerne_size]. \n",
    "\n",
    "- Once your CNN (in Question 6) is trained, access its filters via 'tf.get_collection' and plot them on a grid for each layer. What patterns do you observe, why?\n",
    "- In addition, plot the activations of each layer for two images chosen from digit-classes '2' and '9'. Discuss your observations\n",
    "\n",
    "Hint: \n",
    "+ Activations are for example the outputs of tf.layers.conv2D.\n",
    "+ Use tf.get_collection to access the learned filters of each layers. For this, you need to know how they are named (which can be accessed by tf.trainable_variables among other means of doing so).\n",
    "\n",
    "The plotted feature maps and learned filters should be plotted in the same way as in the images bellow. (left: Feature Maps, right: learned Filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Examples of activation maps (left), and learned features (right))](im.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Multi Task Learning (12 points)\n",
    "\n",
    "This question concerns the practice of multi-task learning (MTL). \n",
    "The aim of multi task learning is to leverage two (or more) related tasks in the learning process with the hope that leanring one task aids perfromance in learning the other task(s) and thus improves predicitve power for at least one (ideally all) of the tasks. \n",
    "\n",
    "There are two distinct flavours of MTL: Hard parameter  sharing and soft parameter sharing. We will be focusing on the former in this question.\n",
    "Hard paramter sharing occurs when two tasks share a common network which then splits into task specific paths (e.g. a series of convolutional layers with two paths of dense layers for two seperate tasks). \n",
    "\n",
    "In this question, you will explore the FASHION MNIST dataset and be coding up your own MTL model and considering the pros and cons of MTL compared to single task learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have written the code to pre-load this dataset and split it into two related tasks for you:\n",
    "\n",
    "- Task 1 - Clothing item 10 class classification (e.g. shoes, t-shirts etc) across 10 goups - $ y \\in \\mathbb{R^{10}} $\n",
    "- Task 2 - Clothing group three class classification - predicitng whether a viewed clothing image belongs to one of three groups - $ y \\in \\mathbb{R^{3}} $\n",
    "    - These groups are shoes (Sandal, Sneaker and Ankle Boot),  Gendered (Dress, Shirt and Bag) and Uni-Sex (T-shirt, Trouser, Pullover and Coat). \n",
    "\n",
    "\n",
    "#### Note : Alternativley use the tf.nn module. \n",
    "\n",
    "#### Note  : We advise the use of only a single epoch for this question for the sake of computation time.  However, if you want to utilise additional epochs feel free to do so just be aware of the longer training time and be consistent over all networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for loading the dataset\n",
    "\"\"\"\n",
    "import keras.datasets.fashion_mnist as fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def load_data(): \n",
    "    # train_X: (60000, 28, 28)\n",
    "    # train_y: (60000,)\n",
    "    # test_X: (10000, 28, 28)\n",
    "    # test_y: (10000,)\n",
    "    (train_X, train_y_1), (test_X, test_y_1) = fashion_mnist.load_data()\n",
    "    n_class_1 = 10\n",
    "    # map to new label\n",
    "    train_y_2 = list(0 if y in [5, 7, 9] else 1 if y in [3, 6, 8] else 2 for y in train_y_1)  \n",
    "    test_y_2 = list(0 if y in [5, 7, 9] else 1 if y in [3, 6, 8] else 2 for y in test_y_1)\n",
    "    n_class_2 = 3\n",
    "    # train_X: (60000, 28, 28, 1)\n",
    "    # test_X: (10000, 28, 28, 1)\n",
    "    # train_y: (60000, n_class = 10)\n",
    "    # test_y: (10000, n_class = 3)\n",
    "    train_X = np.expand_dims(train_X, axis=3)\n",
    "    test_X = np.expand_dims(test_X, axis=3)\n",
    "    train_y_1 = to_categorical(train_y_1, n_class_1)\n",
    "    test_y_1 = to_categorical(test_y_1, n_class_1)\n",
    "    train_y_2 = to_categorical(train_y_2, n_class_2)\n",
    "    test_y_2 = to_categorical(test_y_2, n_class_2)\n",
    "    return train_X, train_y_1, train_y_2, test_X, test_y_1, test_y_2\n",
    "\n",
    "\n",
    "x_train, y_train_1, y_train_2, x_test, y_test_1, y_test_2 = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6.1\n",
    "\n",
    "In this question you will construct two seperate networks of identical structure (excpet the logits and pred layers) one for each of the two tasks.\n",
    "\n",
    "In other words, create a network for item classification and a network for item group classification. \n",
    "\n",
    "\n",
    "Complete the Task_1_NN and Task_2_NN below. These single task networks will form the basis of your work in this question. \n",
    "\n",
    "For the sake of convieience, we will use the same CNN filters as Question 4 - $[32, 64, 128]$. However our kernel size will be 3 $\\times$ 3 and a stride of 1 for all convolutional layers. Maxpooling layers will also need to be implemented after the first and second convolutional layers. These maxpooling layers have a kernel size of two and a stride of 2. \n",
    "\n",
    "After the final convolution, flatten the outputs and pass them to dense layers $[3136, 1024, 100 , N]$ where $N$ is the number of outputs required (10 or 3). \n",
    "\n",
    "As with Question 4, the function 'create_model' to be cpomplemted defines the class variables:\n",
    " - Task 1\n",
    "     - self.logits $\\in \\mathbb{R^{10}}$ containing the output without activation of the last __<font color='red'>fully connected (i.e. dense) layer</font>.__ \n",
    "     - self.preds $\\in \\mathbb{R^{10}}$ containing posterior probabilities.\n",
    " - Task 2\n",
    "      - self.logits $\\in \\mathbb{R^{3}}$ containing the output without activation of the last __<font color='red'>fully connected layer</font>.__ \n",
    "      - self.preds $\\in \\mathbb{R^{3}}$ containing posterior probabilities.\n",
    "      \n",
    "The method definitons remain the same  as in previous questions. \n",
    "      \n",
    "      \n",
    "#### Note: We advise you save the number of parameters and accuracy of the models in order to save time later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task_1_NN():\n",
    "    def __init__(self, x_train, y_train_1,  output_dir, lr=0.001, nb_epochs=10, batch_size=50):\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.nb_images, self.edge, _, _ = x_train.shape\n",
    "        self.nb_iterations = self.nb_images // batch_size\n",
    "        self.output_dir = output_dir\n",
    "        self.x_train = x_train\n",
    "        self.y_train_1 = y_train_1\n",
    "        self.m = x_train.shape[0]\n",
    "        self.n_output_1 = y_train_1.shape[1]\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, (None, 28, 28, 1), \"X\")\n",
    "        self.y_1 = tf.placeholder(tf.float32, (None, self.n_output_1), \"y_1\")\n",
    "    \n",
    "     def create_model(self):            \n",
    "        with tf.variable_scope(\"Task_1\", reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            ######### Complete the function ######### \n",
    "            self.logits = \n",
    "            self.preds =\n",
    "            #########################################\n",
    "                \n",
    "                \n",
    "                \n",
    "    def compute_loss(self):\n",
    "        with tf.variable_scope('loss'):\n",
    "            ######### Complete the function ######### \n",
    "            self.loss_task_1 = \n",
    "            #########################################\n",
    "            self.loss_summ = tf.summary.scalar(\"softmax_loss\", self.loss_task_1) \n",
    "                \n",
    "                \n",
    "    def optimizer(self):\n",
    "        with tf.variable_scope('optimizer', reuse=tf.AUTO_REUSE):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.5)\n",
    "            self.model_vars = tf.trainable_variables()\n",
    "            self.trainer = optimizer.minimize(self.loss_task_1, var_list=self.model_vars)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task_2_NN():\n",
    "    def __init__(self, x_train, y_train_2,  output_dir, lr=0.001, nb_epochs=10, batch_size=50):\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.nb_images, self.edge, _, _ = x_train.shape\n",
    "        self.nb_iterations = self.nb_images // batch_size\n",
    "        self.output_dir = output_dir\n",
    "        self.x_train = x_train\n",
    "        self.y_train_2 = y_train_2\n",
    "        self.m = x_train.shape[0]\n",
    "        self.n_output_2 = y_train_2.shape[1]\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, (None, 28, 28, 1), \"X\")\n",
    "        self.y_2 = tf.placeholder(tf.float32, (None, self.n_output_2), \"y_2\")\n",
    "    \n",
    "     def create_model(self):            \n",
    "        with tf.variable_scope(\"Task_2\", reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            ######### Complete the function ######### \n",
    "            self.logits = \n",
    "            self.preds =\n",
    "            #########################################\n",
    "                \n",
    "                \n",
    "                \n",
    "    def compute_loss(self):\n",
    "        with tf.variable_scope('loss'):\n",
    "            ######### Complete the function ######### \n",
    "            self.loss_task_2 = \n",
    "            #########################################\n",
    "            self.loss_summ = tf.summary.scalar(\"softmax_loss\", self.loss_task_2) \n",
    "                \n",
    "                \n",
    "    def optimizer(self):\n",
    "        with tf.variable_scope('optimizer', reuse=tf.AUTO_REUSE):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.5)\n",
    "            self.model_vars = tf.trainable_variables()\n",
    "            self.trainer = optimizer.minimize(self.loss_task_2, var_list=self.model_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now train and test Task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_1 = Task_1_NN(x_train, y_train_1, './Task1_logdir/', 0.001, 2, 10)            \n",
    "model_1.create_model()     \n",
    "\n",
    "model_1.compute_loss()\n",
    "model_1.optimizer()   \n",
    "\n",
    "model_1.optimizer()\n",
    "init = (tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "summary =tf.Summary()\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "writer = tf.summary.FileWriter(model_1.output_dir)\n",
    "writer.add_graph(sess.graph)\n",
    "if not os.path.exists(model_1.output_dir):\n",
    "    os.makedirs(model_1.output_dir)  \n",
    "\n",
    "\"\"\"\n",
    "Train\n",
    "\"\"\"\n",
    "\n",
    "for epoch in range(model_1.nb_epochs):\n",
    "    randomize = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(randomize)\n",
    "    x_in = model_1.x_train[randomize,:]\n",
    "    y_in_1 = model_1.y_train_1[randomize,:]\n",
    "    for i in range(model_1.nb_iterations):\n",
    "        input_x_train = x_in[i*model_1.batch_size: (i+1)*model_1.batch_size]\n",
    "        input_y_train_1 = y_in_1[i*model_1.batch_size: (i+1)*model_1.batch_size]\n",
    "        _ , preds_1, loss_1, loss_summ = sess.run([model_1.trainer, model_1.pred_1,  model_1.loss_task_1, model_1.loss_summ], \n",
    "                                 feed_dict={model_1.X: input_x_train, \n",
    "                                            model_1.y_1: input_y_train_1})\n",
    "\n",
    "        y_preds_1 = np.argmax(preds_1, axis=1)\n",
    "        y_real_1 = np.argmax(input_y_train_1, axis=1)\n",
    "        acc_train_1 = np.mean((y_preds_1==y_real_1)*1)\n",
    "        print('Epoch %d, Iteration %d, loss_1 %.3f,  batch accuracy_1 %.3f' %(epoch, i, loss_1,acc_train_1))\n",
    "        writer.add_summary(loss_summ, epoch * model_1.nb_iterations + i)\n",
    "    saver.save(sess, model_1.output_dir, global_step=epoch) \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "batch_size_test = 20\n",
    "nb_test_points = x_test.shape[0] \n",
    "nb_iterations = nb_test_points//batch_size_test\n",
    "preds_1 = []\n",
    "for i in range(nb_iterations):\n",
    "    input_x_test = x_test[i*batch_size_test: (i+1)*batch_size_test]\n",
    "    preds_test_1 = sess.run(model_1.pred_1, \n",
    "                             feed_dict={model_1.X: input_x_test})\n",
    "    preds_1.append(np.argmax(preds_test_1, axis=1))\n",
    "    if np.mod(nb_test_points, batch_size_test) !=0:\n",
    "        input_x_test = x_test[i*batch_size_test: -1]\n",
    "        preds_test_1= sess.run(model_1.pred_1, \n",
    "                             feed_dict={model_1.X: input_x_test})\n",
    "        preds_1.append(np.argmax(preds_test_1, axis=1))\n",
    "all_preds_1 = np.concatenate(preds_1, axis =0)\n",
    "y_real_1 = np.argmax(y_test_1, axis=1)\n",
    "print(all_preds_1)\n",
    "print(y_real_1)\n",
    "acc_test_1 = np.mean((all_preds_1==y_real_1)*1)\n",
    "print('Test accuracy - task 1 achieved: %.3f' %acc_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now train and test Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Task_2_NN(x_train, y_train_2, './Task2_logdir/', 0.001, 2, 10)            \n",
    "model_2.create_model()     \n",
    "\n",
    "model_2.compute_loss()\n",
    "model_2.optimizer()   \n",
    "\n",
    "model_2.optimizer()\n",
    "init = (tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "summary =tf.Summary()\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "writer = tf.summary.FileWriter(model_2.output_dir)\n",
    "writer.add_graph(sess.graph)\n",
    "if not os.path.exists(model_2.output_dir):\n",
    "    os.makedirs(model_2.output_dir)\n",
    "\n",
    "\"\"\"\n",
    "Train\n",
    "\"\"\"\n",
    "for epoch in range(model_2.nb_epochs):\n",
    "    randomize = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(randomize)\n",
    "    x_in = model_2.x_train[randomize,:]\n",
    "    y_in_2 = model_2.y_train_2[randomize,:]\n",
    "    for i in range(model_2.nb_iterations):\n",
    "        input_x_train = x_in[i*model_2.batch_size: (i+1)*model_2.batch_size]\n",
    "        input_y_train_2 = y_in_2[i*model_2.batch_size: (i+1)*model_2.batch_size]\n",
    "        _ , preds_2, loss_2, loss_sum = sess.run([model_2.trainer, model_2.pred_2,  model_2.loss_task_2, model_2.loss_sum], \n",
    "                                 feed_dict={model_2.X: input_x_train, \n",
    "                                            model_2.y_2: input_y_train_2})\n",
    "        y_preds_2 = np.argmax(preds_2, axis=1)\n",
    "        y_real_2 = np.argmax(input_y_train_2, axis=1)\n",
    "        acc_train_2 = np.mean((y_preds_2==y_real_2)*1)\n",
    "        print('Epoch %d, Iteration %d, loss_2 %.3f, batch accuracy_2 %.3f' %(epoch, i, loss_2,acc_train_2))\n",
    "        writer.add_summary(loss_sum, epoch * model_2.nb_iterations + i)\n",
    "    saver.save(sess, model_2.output_dir, global_step=epoch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test\n",
    "\"\"\"\n",
    "batch_size_test = 20\n",
    "nb_test_points = x_test.shape[0]\n",
    "nb_iterations = nb_test_points//batch_size_test\n",
    "preds_2 = []\n",
    "for i in range(nb_iterations):\n",
    "    input_x_test = x_test[i*batch_size_test: (i+1)*batch_size_test]\n",
    "    preds_test_2 = sess.run(model_2.pred_2, \n",
    "                             feed_dict={model_2.X: input_x_test})\n",
    "    preds_2.append(np.argmax(preds_test_2, axis=1))\n",
    "    if np.mod(nb_test_points, batch_size_test) !=0:\n",
    "        input_x_test = x_test[i*batch_size_test: -1]\n",
    "        preds_test_2= sess.run([model_2.pred_2], \n",
    "                             feed_dict={model_2.X: input_x_test})\n",
    "        preds_2.append(np.argmax(preds_test_2, axis=1))\n",
    "all_preds_2 = np.concatenate(preds_2, axis =0)\n",
    "y_real_2 = np.argmax(y_test_2, axis=1)\n",
    "acc_test_2 = np.mean((all_preds_2==y_real_2)*1)\n",
    "\n",
    "print('Test accuracy - task 2 achieved: %.3f' %acc_test_2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6.2 - Building a MTL Network\n",
    "\n",
    "In this question, we wish for you to complete the MTL class below and use it to train and test a MTL model on the two MNIST fashion tasks. \n",
    "\n",
    "Similar to Questions 3 and 4, our MTL class is initiliased via\n",
    " - x_train, the training matrix\n",
    " - y_train_1, the fashion labels for task 1 (Fashion Item  classification)\n",
    " - y_train_2, the labels for task 2 (Fashion Group classification)\n",
    " - $\\lambda \\in [0,1]$ , lambda_, the loss weight for task 1 (1- $\\lambda$) is the loss weight for task 2\n",
    " - output_dir, the directory where model parameters and tensorbaord event files will be stored. \n",
    " - lr, the learning rate of the ADAM optimiser \n",
    " - nb_epochs, the number of epochs to use\n",
    " - batch_size, the number of data points in each mini-batch\n",
    "\n",
    "Our MTL architecture will be comprised of a shared CNN backbone of three convolutional layers and a single shared dense layer with pooling between the first two pairs of convolutions.  The output of the shared dense layer is passed to two series of task specific dense layers, one for each of the two tasks. \n",
    "\n",
    "The architecture is as follows:\n",
    " - Shared Convolutional layers $[32, 64, 128]$ with max pooling after the first and second conv layers\n",
    "     - kernel size ($ 3 \\times 3$) for conv and ($2 \\times 2$) for max pool\n",
    "     - stride 1 for conv and 2 for max pooling\n",
    " - Flatten \n",
    " - Shared Dense Layer $[3136]$ - the outputs of which are passed to the two task dense layers\n",
    " - Task 1 Dense Layers $[1024, 100, 10]$ - 10 is the dimenson of the logits/preds\n",
    " - Task 2 Dense Layers $[1024, 100, 3]$ - 3 is the dimenson of the logits/preds \n",
    " - Task 1 Activation Layer - as earlier we use softmax\n",
    " - Task 2 Activation Layer  - as earlier we use softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture is illustrated in the figure below minus the flattening layer.\n",
    "\n",
    "\n",
    "<img src=\"mtl_2.jpg\" alt=\"The MTL network\" title=\"MTL Architecture\" />\n",
    "\n",
    " \n",
    " The function 'create_model' defines the class variables:\n",
    "  - self.logits_1 $\\in \\mathbb{R^{10}}$ containing the output without activation of the last <font color='red'>fully connected layer</font> of the Task 1 task layers. \n",
    "  - self.logits_2 $\\in \\mathbb{R^{3}}$ containing the output without activation of the last <font color='red'>fully connected layer</font> of the Task 2 task layers\n",
    "  - self.preds_1 $\\in \\mathbb{R^{10}}$ containing posterior probabilities for the first task.\n",
    "  - self.preds_2 $\\in \\mathbb{R^{3}}$ containing posterior probabilities for the second task.\n",
    "  \n",
    "As above, use self.logits to complete the method 'compute_loss' that takes the labels and the predicted logits to return the corresponfing cross-entropy loss albeit for each task. \n",
    "\n",
    "The total loss which is a sum of the weighted losses from tasks 1 and 2 ($\\lambda * L_1 + (1-\\lambda) * L_2$) is passed to the optimiser. \n",
    "\n",
    "#### For this question set $\\lambda$ to be 0.5 for equal weighting. \n",
    "\n",
    " #### Note: Do not worry about the optimiser - we still only need one optimiser for joint training of the MTL network \n",
    "      - The tasks can be trained alternately but this has its drawbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL:\n",
    "    def __init__(self, x_train, y_train_1, y_train_2, lambda_, output_dir, lr=0.001, nb_epochs=10, batch_size=50):\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.nb_images, self.edge, _, _ = x_train.shape\n",
    "        self.nb_iterations = self.nb_images // batch_size\n",
    "        self.output_dir = output_dir\n",
    "        self.x_train = x_train\n",
    "        self.y_train_1 = y_train_1\n",
    "        self.y_train_2 = y_train_2\n",
    "        self.lambda_ = lambda_\n",
    "        \n",
    "        self.m = x_train.shape[0]\n",
    "        self.n_output_1 = y_train_1.shape[1]\n",
    "        self.n_output_2 = y_train_2.shape[1]\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, (None, 28, 28, 1), \"X\")\n",
    "        self.y_1 = tf.placeholder(tf.float32, (None, self.n_output_1), \"y_1\")\n",
    "        self.y_2 = tf.placeholder(tf.float32, (None, self.n_output_2), \"y_2\")\n",
    "\n",
    "    \n",
    "    def create_model(self):            \n",
    "        with tf.variable_scope(\"MTL\", reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            ### Complete the function #####\n",
    "            \n",
    "            self.logits_1 = \n",
    "            self.pred_1 = \n",
    "            \n",
    "            self.logits_2 = \n",
    "            self.pred_2 = \n",
    "        \n",
    "    def compute_loss(self):\n",
    "        with tf.variable_scope('loss'):\n",
    "            ######### Complete the function ######### \n",
    "            self.loss_task_1 =\n",
    "\n",
    "            self.loss_task_2 = \n",
    "\n",
    "            self.loss_total = self.lambda_*self.loss_task_1 + (1-self.lambda_)*self.loss_task_2 \n",
    "            #########################################\n",
    "            self.loss_task_1_graph = tf.summary.scalar(\"softmax_loss_task_1\", self.loss_task_1) \n",
    "            self.loss_task_2_graph = tf.summary.scalar(\"softmax_loss_task_2\", self.loss_task_2)             \n",
    "            self.loss_sum = tf.summary.scalar(\"softmax_loss\", self.loss_total) \n",
    "            \n",
    "                \n",
    "                \n",
    "    def optimizer(self):\n",
    "        with tf.variable_scope('optimizer', reuse=tf.AUTO_REUSE):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.5)\n",
    "            self.model_vars = tf.trainable_variables()\n",
    "            self.trainer = optimizer.minimize(self.loss_total, var_list=self.model_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create model and initialise it and tensorflow session\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = MTL(x_train, y_train_1, y_train_2, 0.5, './MTL_logdir/', 0.001, 2, 10)             #       \n",
    "model.create_model()     \n",
    "\n",
    "model.compute_loss()\n",
    "model.optimizer()   \n",
    "\n",
    "model.optimizer()\n",
    "init = (tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "summary =tf.Summary()\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "writer = tf.summary.FileWriter(model.output_dir)\n",
    "writer.add_graph(sess.graph)\n",
    "if not os.path.exists(model.output_dir):\n",
    "    os.makedirs(model.output_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now train and test your MTL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(model.nb_epochs):\n",
    "    randomize = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(randomize)\n",
    "    x_in = model.x_train[randomize,:]\n",
    "    y_in_1 = model.y_train_1[randomize,:]\n",
    "    y_in_2 = model.y_train_2[randomize,:]\n",
    "    for i in range(model.nb_iterations):\n",
    "        input_x_train = x_in[i*model.batch_size: (i+1)*model.batch_size]\n",
    "        input_y_train_1 = y_in_1[i*model.batch_size: (i+1)*model.batch_size]\n",
    "        input_y_train_2 = y_in_2[i*model.batch_size: (i+1)*model.batch_size]\n",
    "        _ , preds_1, preds_2, loss_1, loss_2, loss_summ= sess.run([model.trainer, model.pred_1, model.pred_2, model.loss_task_1, model.loss_task_2, model.loss_sum], \n",
    "                                 feed_dict={model.X: input_x_train, \n",
    "                                            model.y_1: input_y_train_1,\n",
    "                                            model.y_2: input_y_train_2})\n",
    "\n",
    "        y_preds_1 = np.argmax(preds_1, axis=1)\n",
    "        y_preds_2 = np.argmax(preds_2, axis=1)\n",
    "        y_real_1 = np.argmax(input_y_train_1, axis=1)\n",
    "        y_real_2 = np.argmax(input_y_train_2, axis=1)\n",
    "        acc_train_1 = np.mean((y_preds_1==y_real_1)*1)\n",
    "        acc_train_2 = np.mean((y_preds_2==y_real_2)*1)\n",
    "        print('Epoch %d, Iteration %d, loss_1 %.3f, loss_2 %.3f, batch accuracy_1 %.3f, batch accuracy_2 %.3f' %(epoch, i, loss_1, loss_2, acc_train_1, acc_train_2))\n",
    "        writer.add_summary(loss_summ, epoch * model.nb_iterations + i)\n",
    "    saver.save(sess, model.output_dir, global_step=epoch) \n",
    "end = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test\n",
    "\"\"\"\n",
    "batch_size_test = 20\n",
    "nb_test_points = x_test.shape[0] \n",
    "nb_iterations = nb_test_points//batch_size_test\n",
    "preds_1 = []\n",
    "preds_2 = []\n",
    "for i in range(nb_iterations):\n",
    "    input_x_test = x_test[i*batch_size_test: (i+1)*batch_size_test]\n",
    "    preds_test_1, preds_test_2 = sess.run([model.pred_1, model.pred_2], \n",
    "                             feed_dict={model.X: input_x_test})\n",
    "    preds_1.append(np.argmax(preds_test_1, axis=1))\n",
    "    preds_2.append(np.argmax(preds_test_2, axis=1))\n",
    "    if np.mod(nb_test_points, batch_size_test) !=0:\n",
    "        input_x_test = x_test[i*batch_size_test: -1]\n",
    "        preds_test_1, preds_test_2 = sess.run([model.pred_1, model.pred_2], \n",
    "                             feed_dict={model.X: input_x_test})\n",
    "        preds_1.append(np.argmax(preds_test_1, axis=1))\n",
    "        preds_2.append(np.argmax(preds_test_2, axis=1))\n",
    "all_preds_1 = np.concatenate(preds_1, axis =0)\n",
    "all_preds_2 = np.concatenate(preds_2, axis =0)\n",
    "y_real_1 = np.argmax(y_test_1, axis=1)\n",
    "y_real_2 = np.argmax(y_test_2, axis=1)\n",
    "acc_test_1 = np.mean((all_preds_1==y_real_1)*1)\n",
    "acc_test_2 = np.mean((all_preds_2==y_real_2)*1)\n",
    "print('Test accuracy - task 1 achieved: %.3f' %acc_test_1)\n",
    "print('Test accuracy - task 2 achieved: %.3f' %acc_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss the performance of the MTL network compared to the single task networks. What do think are the most important things to consider when using MTL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 6.3\n",
    "\n",
    "In this task we want you to compare the MTL's results for different values of $\\lambda$. We want you to coampre $5$ different values of $\\lambda$ with the stipulation that two of them must be $0$ and $1$; the other 3 are up to you. All other hyperparameters (e.g. number of dense shared layers) can be left as they were in Task 6.2. \n",
    "\n",
    "Plot a graph showing the Task 1 and Task 2 accuracies against values of $\\lambda$. \n",
    "Comment on your results. \n",
    "Whats so special about the cases of $\\lambda = 0$ and $\\lambda = 1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Task 6.4\n",
    "\n",
    "##### Similar to tasks  3.2 and 4.2\n",
    "\n",
    " - Plot a graph showing complexity (number of paramters) vs. accuracy for each of the to tasks in the MTL network. As above you will need to train four other additional MTL networks where you vary the number/size of the shared and  task specific layers (we advise not making the layers too large or too numerous to avoid very long training times). \n",
    " - You should also include plots of the number of parameters vs accuracy for the individual task 1 and 2 networks - use different colours to distinguish these single task networks.\n",
    "     - Note 1 : There is no need to train additional Task 1 and Task 2 networks simply use the formats from Question 6.1\n",
    "     - Note 2: For all MTL formats set $\\lambda$ to $0.5$ for equal weighting\n",
    "     - Note 3: Due to the sub task structure, we will ignore depth in this evaluation as the two tasks can have different depth levels in the MTL architecture. \n",
    "     - Note 4: It is advised to include pooling layers (we leave the type of pooling up to you but recommned maxpooling) for the sake of training time - alternativly use larger strides. \n",
    " \n",
    " - Discuss the effect of the number/size of the shared and task specific layers in terms of performance and training/testing times. Also compare the performance to the task specific networks from 6.1. \n",
    " \n",
    " - Discuss the main pros and cons for multi-task learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Denoising Auto-Encoder (10 points)\n",
    "\n",
    "While CNNs make very good classifiers, they can also be used for many various tasks such as regression, image generation, image reconstruction and so on. In this part of the coursework you should implement a fully convolutional de-noising auto-encoder, i.e. a network that takes a noisy image as input and outputs the corresponding cleaned output. In this exercise we will introduce a gaussian noise on each input. Our goal is to recover the original noiseless images in the output. We will be using the MNIST dataset here.\n",
    "\n",
    "Similarly as before we will define the class DAE, and you will need to complete the method 'create_model' and 'compute_loss' accordingly. \n",
    "\n",
    "Using the functions tf.layers.conv2d and tf.layers.conv2d_transpose complete the method 'create_model' in the class DAE below. \n",
    "\n",
    "conv2d should be used for the encoding part while conv2d_transpose should be used for the decoding part. \n",
    "We will use ReLu as the non linear activation for the hidden layers and tanh as the activation for the output layer. \n",
    "The network architecture is as follows:\n",
    "\n",
    "+ Encoding part: a series of conv2d layers with [32, 64, 128] filters of size 4\\times4, a stride of 1 in the first layer and a stride of 2 for the second and third layers.  \n",
    "+ Decodind part: a series of two conv2d_transpose followed by one, one strided conv2d with [64,32,1] filters of size 4\\times4. \n",
    "\n",
    "The output dimension of the final layer should match the input dimension [batch_size, 28, 28]. If necesseray use the argument padding or the function tf.pad to make the input and ouput dimensions match. \n",
    "\n",
    "Complete the method compute_loss. Note that we don't have a classification problem anymore but a regression problem consisting of reconstructing the noiseless version of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAE:\n",
    "    def __init__(self, x_train, output_dir, lr=0.001, nb_epochs=10, batch_size=50):\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.nb_images, self.edge, _ = x_train.shape\n",
    "        self.nb_iterations = self.nb_images // batch_size\n",
    "        self.output_dir = output_dir\n",
    "        self.im = tf.placeholder(tf.float32, [None, 28, 28,1])\n",
    "        self.im_n = self.im + tf.random_normal(tf.shape(self.im), mean=0, stddev=1) #inject noise\n",
    "        self.x_train = x_train\n",
    "        \n",
    "    def create_model(self):\n",
    "        with tf.variable_scope('DAE', reuse=tf.AUTO_REUSE):\n",
    "            ######### Complete the function ######### \n",
    "            self.recon_im = \n",
    "            #########################################\n",
    "            \n",
    "            tf.summary.image('denoising', tf.concat([self.im_n, self.recon_im], axis=2)\n",
    "    \n",
    "    def compute_loss(self):\n",
    "        with tf.variable_scope('loss'):\n",
    "            ######### Complete the function ######### \n",
    "            self.loss\n",
    "            #########################################\n",
    "            self.loss_summ = tf.summary.scalar(\"reconstruction_loss\", self.loss)\n",
    "                             \n",
    "    def optimizer(self):\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.5)\n",
    "            self.model_vars = tf.trainable_variables()\n",
    "            self.trainer = optimizer.minimize(self.loss, var_list=self.model_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DAE(x_train, './DAE_logdir/', 0.001, 2, 10)\n",
    "model.create_model()\n",
    "model.compute_loss()\n",
    "model.optimizer()\n",
    "init = (tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "summary =tf.Summary()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "writer = tf.summary.FileWriter(model.output_dir)\n",
    "writer.add_graph(sess.graph)\n",
    "if not os.path.exists(model.output_dir):\n",
    "    os.makedirs(model.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous questions, train the DAE by looping over all the mini-batches. For proper training you should input enough noisy versions per image. \n",
    "- Plot the training/validation curves and discuss why your trained model is not under/over fitting.\n",
    "When the training is finished, inject noise into the test data and reconstruct them through a forward pass. For each test image, input 20 noisy versions of it and compute the average accuracies. \n",
    "\n",
    "- Test the robustness of the trained network to different levels of noise, demonstrate the results (figure or Table) and discuss them in your report. \n",
    "- Why this architecture is able to remove noise from data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 - Building a Variational AutoEncoder (VAE) 12 Points\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Why use a VAE?\n",
    "\n",
    "<br>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Building on the standard AutoEncoder which was a way of learning some lower dimensional manifold the data could lie on, we want to impose a form of regularisation, that is, we want some topology structure. We want inputs to the AutoEncoder that have a certain structure to be mapped to similar areas in this lower dimensional latent space. But the standard AutoEncoder merely learns how to essentially compress data. In this task, we are interested in learning a more structured latent space where we can sample in this space to then generate a new image that theoretically belongs to the same dataset.\n",
    "    \n",
    "In this exercise, we will assume the structure in the latent space follows a standard bi-variate Gaussian i.e. mean $\\begin{bmatrix}0\\\\0\\end{bmatrix}$ and covariance $\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}$. This assumption is for simplicity but we can infact assume other distributions (outside the scope for this exercise). We will be using the MNIST dataset for this task.\n",
    "    \n",
    "</p>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "<img src=\"vae-diagram.png\" alt=\"The Variational AutoEncoder\" title=\"VAE Architecture\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Notation\n",
    "\n",
    "<br>\n",
    "\n",
    "Before diving into some maths, lets define a few terms:\n",
    "1. $\\mathbf{X}$: the dataset distribution we wish to generate from\n",
    "2. $\\mathbf{z}$: the compressed latent variable which quantifies a set of attributes of our dataset\n",
    "3. $\\boldsymbol{\\theta}$ : the parameters for the ***encoder*** network\n",
    "4. $\\boldsymbol{\\phi}$: the parameters for the ***decoder*** network\n",
    "5. $p(\\mathbf{z})$: the probability of the compressed latent space (given it should follow a standard normal)\n",
    "6. $q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{X})$: the conditional probability distribution of sampling the compressed latent variable given the dataset approximated by our encoder network with parameters $\\boldsymbol{\\phi}$\n",
    "7. $p_{\\boldsymbol{\\theta}}(\\mathbf{X}|\\mathbf{z})$: the conditional probability distribution of generating the dataset given the compressed latent space by our decoder network with parameters $\\boldsymbol{\\theta}$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Optimisation\n",
    "\n",
    "<br>\n",
    "\n",
    "The quantity we are interested in maximising for the $i$-th observation is:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "    \\log p (\\mathbf{x}_i) && \\text{which can be rewritten as...}\\\\\n",
    "    &= \\int_{\\mathbf{z}\\in\\mathcal{Z}} q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}_i) \\log p (\\mathbf{x}_i) \\text{d}\\mathbf{z} & \\text{as the integral over a distribution is 1...}\\\\\n",
    "    &= \\int_{\\mathbf{z}\\in\\mathcal{Z}} q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}_i)[\\log p (\\mathbf{z}|\\mathbf{x}_i) - \\log p(\\mathbf{z}) + \\log p_\\boldsymbol{\\theta}(\\mathbf{x}_i|\\mathbf{z})]\\text{d}\\mathbf{z} & \\text{consider Baye's rule...}\\\\\n",
    "    &= \\int_{\\mathbf{z}\\in\\mathcal{Z}} q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}_i) \\log \\frac{p (\\mathbf{z}|\\mathbf{x}_i)}{q_{\\boldsymbol{\\phi}} (\\mathbf{z}|\\mathbf{x}_i)} - q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}_i)\\log \\frac{p (\\mathbf{z})}{q_{\\boldsymbol{\\phi}} (\\mathbf{z}|\\mathbf{x}_i)} + q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}_i) \\log p_\\boldsymbol{\\theta}(\\mathbf{x}_i|\\mathbf{z})\\text{d}\\mathbf{z}\\\\\n",
    "    &= \\text{D}_{\\text{KL}}(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}_i) || p (\\mathbf{z}|\\mathbf{x}_i)) - \\text{D}_{\\text{KL}}(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}_i) || p (\\mathbf{z})) + \\mathbb{E}_{q_{\\boldsymbol{\\phi}} (\\mathbf{z}|\\mathbf{x}_i)} [\\log p_{\\boldsymbol{\\theta}} (\\mathbf{x}_i|\\mathbf{z})]\\\\\n",
    "    &=  \\text{D}_{\\text{KL}}(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}_i) || p (\\mathbf{z}|\\mathbf{x}_i)) + \\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\mathbf{x}_i) &\\text{where the last two terms have been grouped as $\\mathcal{L}$}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "As the first term in the line above is intractable to compute, we can say that $\\log p_\\boldsymbol{\\theta} (\\mathbf{x}_i) \\ge \\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\mathbf{x}_i)$ as the KL-divergence is a distance measure (and hence non-negative) and $\\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\mathbf{x}_i)$ is known as the **variational lower bound**. Instead of maximising the log probability directly, we can instead maximise this lower bound! If you are unfamiliar with the KL-divergence, check out this [Wikipedia](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n",
    "\n",
    "<br>\n",
    "\n",
    "The lower bound can be clearly seen as two main components:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\mathbf{x}_i) = \\color{blue}{\\mathbb{E}_{q_{\\boldsymbol{\\phi}} (\\mathbf{z}|\\mathbf{x}_i)} [\\log p_{\\boldsymbol{\\theta}} (\\mathbf{x}_i|\\mathbf{z})]} - \\color{green}{\\text{D}_{\\text{KL}}(q_{\\boldsymbol{\\phi}}(\\mathbf{z}|\\mathbf{x}_i) || p (\\mathbf{z}))}\n",
    "\\end{align}\n",
    "$\n",
    "+ <font color='blue'> How well can we reconstruct our original data from the latent space?</font>\n",
    "+ <font color='green'> How similar is the latent space to a standard Gaussian?</font>\n",
    "\n",
    "A remark on the second point - we are trying to find a Gaussian distribution manifold in latent space for our dataset. This is known as a *variational* method which should be part of the Bayesian Machine Learning module.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Derivations\n",
    "\n",
    "Given that $\\mathbf{\\hat{x}}_i$ is a draw from $p_{\\boldsymbol{\\theta}} (\\mathbf{x}_i|\\mathbf{z})\\sim\\mathcal{N}(\\mathbf{x},\\sigma^2\\mathbf{I})$, the expectation of $\\log p_{\\boldsymbol{\\theta}} (\\mathbf{x}_i|\\mathbf{z})$ can be computed as:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{q_{\\boldsymbol{\\phi}} (\\mathbf{z}|\\mathbf{x}_i)} [\\log p_{\\boldsymbol{\\theta}} (\\mathbf{x}_i|\\mathbf{z})] &= -\\frac{m}{2}\\log 2\\pi\\sigma^2 - \\frac{1}{2}(\\mathbf{\\hat{x}}_i - \\mathbf{x}_i)^\\text{T}(\\sigma^2\\mathbf{I})^{-1}(\\mathbf{\\hat{x}}_i - \\mathbf{x}_i)\\\\\n",
    "\\mathbb{E}_{q_{\\boldsymbol{\\phi}} (\\mathbf{Z}|\\mathbf{X})} [\\log p_{\\boldsymbol{\\theta}} (\\mathbf{X}|\\mathbf{Z})] &\\propto \\sum_{i = 1}^{i = n}\\sum_{j = 1}^{j = m} (\\hat{x}_{ij} - x_{ij})^2 & \\text{as everything else is just a constant}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "<font color='red'>Given that $q_{\\boldsymbol{\\phi}} (\\mathbf{z}|\\mathbf{x}_i) \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2\\mathbf{I})$, derive the KL-divergence term between this and $p_\\boldsymbol{\\theta} (\\mathbf{z})$ i.e. $\\text{D}_{\\text{KL}} (\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2\\mathbf{I}), \\mathcal{N}(0, \\mathbf{I}))$:</font>\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\text{D}_{\\text{KL}} (\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2\\mathbf{I}), \\mathcal{N}(0, \\mathbf{I})) &= \\int_{\\mathbf{z}\\in\\mathcal{Z}} ?\\text{ d}\\mathbf{z}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Further readings: There are many useful online tutorials that provide deep insight as how VAEs work or to implement them; for instace [tutorial1](https://arxiv.org/abs/1606.05908) [tutorial2](http://ruishu.io/2018/03/14/vae/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Data and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "image_size       = x_train.shape[1]\n",
    "original_dim     = image_size * image_size\n",
    "\n",
    "# Flatten each image to be a vector (you can try convolutional layers as an extension)\n",
    "x_train          = np.reshape(x_train, [-1, original_dim])\n",
    "x_test           = np.reshape(x_test, [-1, original_dim])\n",
    "x_train          = x_train.astype('float32') / 255\n",
    "x_test           = x_test.astype('float32') / 255\n",
    "\n",
    "# Visualisation parameters (for after the implementation of the VAE - start playing with these after the VAE seems to work!)\n",
    "n                = 29 # Number of digits to show per row / col\n",
    "dsize            = 28 # Digit size\n",
    "\n",
    "z1               = norm.ppf(np.linspace(0.01, 0.99, n))\n",
    "z2               = norm.ppf(np.linspace(0.01, 0.99, n))\n",
    "z_grid           = np.dstack(np.meshgrid(z1, z2))\n",
    "\n",
    "Zf               = lambda x : sess.run(z_mean, feed_dict = {t_X : x})\n",
    "Xf               = lambda z_grid : sess.run(t_X_hat, feed_dict = {z : z_grid.reshape(n * n, nlatent)}).reshape(n, n, dsize, dsize)\n",
    "\n",
    "def get_batch(*args, size):\n",
    "    \"\"\" Loops through each argument in batches of [size] \"\"\"\n",
    "    \n",
    "    n = len(args[0])\n",
    "    if size is None or size >= n:\n",
    "        yield from args\n",
    "        return None\n",
    "    r = np.random.permutation(n)\n",
    "    for i in range(n // size + 1):\n",
    "        yield (arg[r[i * size : (i + 1) * size]] for arg in args)\n",
    "        \n",
    "def visualise(X, y, sep = 2):\n",
    "     \"\"\" Visualise the mapped 2D manifold \"\"\"\n",
    "    # Feel free to modify this code for your visualisations...\n",
    "    \n",
    "    Z  = Zf(x_test)\n",
    "    Xh = Xf(z_grid)\n",
    "    \n",
    "    plt.figure(figsize = (12, 10))\n",
    "    plt.scatter(Z[:, 0], Z[:, 1], c = y)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    \n",
    "    plt.figure(figsize = (12, 10))\n",
    "    plt.imshow(np.block(list(map(list, Xh))), cmap = 'gray')\n",
    "    start_range    = dsize // 2\n",
    "    end_range      = n * dsize + start_range\n",
    "    pixel_range    = np.arange(start_range, end_range, dsize)\n",
    "    sample_range_x = np.round(z1, 2)\n",
    "    sample_range_y = np.round(z2, 2)\n",
    "    plt.xticks(pixel_range[::sep], sample_range_x[::sep])\n",
    "    plt.yticks(pixel_range[::sep], sample_range_y[::sep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8.1 Implementation of VAE\n",
    "\n",
    "<br>\n",
    "\n",
    "The simpler implementation of the VAE is to build off the MLP i.e. no convolutional layers. The code below is designed for this but feel free to swap out this assumption for convolutions should you feel more confident. Feel free to add `tf.summary` definitions in a similar way to previous tasks for tensorboard visualisations.\n",
    "\n",
    " #### Note: If implementing task 8.1 in TF (as guided bellow) is not straightforward for you, use an easier option: [Keras](https://keras.io/examples/variational_autoencoder/) implementation of VAE. In these case, you will be marked down 4 points but you can immediately proceed to task 8.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters...\n",
    "layers   = [512, 256, 128]\n",
    "nlatent  = 2\n",
    "batch    = 64\n",
    "epochs   = 20\n",
    "alpha    = 1e-3\n",
    "\n",
    "# General dtype to use\n",
    "dtype    = 'float32'\n",
    "\n",
    "# Number of features\n",
    "m        = x_train.shape[1]\n",
    "\n",
    "# Reset the graph to ensure blank graph initially...\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define placeholder for 'X' data\n",
    "t_X      = tf.placeholder(dtype = dtype, shape = [None, m], name = 'X')\n",
    "\n",
    "# Define how t_X maps to the layer before the latent space\n",
    "with tf.name_scope('Encoder'):\n",
    "    h        = t_X\n",
    "    for i, layer in enumerate(layers, 1):\n",
    "        with tf.name_scope(f'Layer_{i}'):\n",
    "            ######### Map how we get from the previous layer to the next hidden layer #########\n",
    "            h    = \n",
    "    \n",
    "# Define how we get the mean and variance from the previous hidden layer\n",
    "with tf.name_scope('Latent'):\n",
    "    with tf.name_scope('Mean'):\n",
    "        z_mean   = \n",
    "\n",
    "    with tf.name_scope('Variance'):\n",
    "        z_var    = \n",
    "        \n",
    "    # Code the \"reparameterisation trick\"\n",
    "    with tf.name_scope('Sample'):\n",
    "        epsilon  = \n",
    "        z_sample = \n",
    "        z        = tf.placeholder_with_default(z_sample, shape = [None, nlatent], name = 'z')\n",
    "    \n",
    "# Define how \"z\" decodes to our reconstructed \"t_X\" estimate\n",
    "with tf.name_scope('Decoder'):\n",
    "    h        = z\n",
    "    for i, layer in enumerate(layers[::-1], 1):\n",
    "        with tf.name_scope(f'Layer_{i}'):\n",
    "            h    = \n",
    "    \n",
    "    with tf.name_scope('Final'):\n",
    "        t_X_hat  = \n",
    "\n",
    "# Define the loss function as defined in the derivation section\n",
    "with tf.name_scope('Loss'):\n",
    "    with tf.name_scope('AutoEncoder'):\n",
    "        # The normal AutoEncoder loss should measure how far our t_X_hat is from t_X\n",
    "        loss_ae  = \n",
    "        \n",
    "    with tf.name_scope('KL_Divergence'):\n",
    "        # The KL-divergence between z and a standard normal you derived earlier\n",
    "        loss_kl  = \n",
    "        \n",
    "    loss = tf.reduce_mean(loss_ae + loss_kl, name = 'loss')\n",
    "\n",
    "# Define final components before training the model\n",
    "optim    = tf.train.AdamOptimizer(alpha).minimize(loss)\n",
    "train    = {t_X : x_train}\n",
    "\n",
    "sess     = tf.InteractiveSession()\n",
    "writer   = tf.summary.FileWriter('./VAE_MLP_logdir', sess.graph)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Gradient descent loop with verbose loss check...\n",
    "# Hopefully we see our loss reduce!\n",
    "for i in range(epochs):\n",
    "    losses  = []\n",
    "    for xb, in get_batch(x_train, size = batch):\n",
    "        nb = len(xb)\n",
    "        sess.run(optim, feed_dict = {t_X : xb})\n",
    "        losses.append(nb * sess.run(loss, feed_dict = {t_X : xb}))\n",
    "        print(f'\\rIteration {i:2d}: loss = {losses[-1] / nb:6,.2f}', end = '')\n",
    "    print(f'\\rIteration {i:2d}: loss = {sum(losses) / len(x_train):6,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Task 8.2\n",
    "\n",
    "<br>\n",
    "\n",
    " - Plot the learning curve for your VAE\n",
    " - Generate new handwritten digits from your model by sampling `Z` = $\\mathbf{z}\\in\\mathbb{R}^{n,2}$ and running `sess.run(t_X_hat, feed_dict = {z : Z))` in a separate code cell\n",
    "     - Discuss how good / bad are these images?\n",
    "         - Why are they good / bad? (consider the assumptions of your implementation of the VAE)\n",
    " - From observing your learnt manifold (by using the `visualise` function or otherwise), draw a comparison between the direct mapped points on your manifold (scatter plot) and the images that lay on the manifold (imshow)\n",
    "     - Why are some points images more mixed than others?\n",
    "     - Can we be smarter in where we should sample to get *better* generated handwritten digits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
